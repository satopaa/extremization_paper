%\documentclass[11pt, twoside]{article}
\documentclass[11pt]{article}

\usepackage{amsmath} 
\usepackage{times}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{moreverb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amssymb}
\usepackage{url}
\usepackage{multirow} 
\usepackage[boxed, section]{algorithm}
\usepackage{algorithmic}
\usepackage{cite}
\usepackage{multirow} 
\usepackage{rotating}
\usepackage{geometry}
\usepackage{fix-cm}
\usepackage{natbib}
 \usepackage{setspace}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}

\newcommand{\myN}{\hbox{N\hspace*{-.9em}I\hspace*{.4em}}}
\newcommand{\myZ}{\hbox{Z}^+}
\newcommand{\myR}{\hbox{R}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\COR}{\text{COR}}
\newtheorem{defi}{Definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Observation}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\DeclareMathOperator*{\argmax}{arg\,max}

\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\renewcommand{\abstractname}{}
\def\pb{\overline{p}}
\def\pt{\tilde{p}}
\def\one{{\bf 1}}
\def\v{{\bf v}}
\def\F{{\cal F}}
\def\G{{\cal G}}
\def\P{{\mathbb P}}
\def\E{{\mathbb E}}
\def\Var{{\rm Var}\,}
\def\Cov{{\rm Cov}\,}
\def\ee{\varepsilon}
\def\|{\, | \,}
\def\probit{p_{\rm probit}}
\def\plog{p_{\rm log}}

%\doublespacing

%%%%%% Begin document with header and title %%%%%%%%%

%\title{Combining Probability Forecasts and Understanding Probability Extremizing through Information Diversity}
\title{Modeling Probability Forecasts via Information Diversity}
\author[1]{Ville A. Satop\"a\"a\thanks{Corresponding author. tel.: +1 215 760 7263; fax: +1 215 898 1280; email: satopaa@wharton.upenn.edu}}
%\author[2]{Robin Pemantle\thanks{pemantle@math.upenn.edu}}
%\thanks{Research supported by NSF award \# DMS-1209117}
%\author[3]{Lyle H. Ungar\thanks{ungar@cis.upenn.edu}}
\author[2]{Robin Pemantle}
\author[3]{Lyle H. Ungar}
\affil[1]{Department of Statistics,
The Wharton School of the University of Pennsylvania\\
400 Jon M. Huntsman Hall\\
3730 Walnut Street\\
Philadelphia, PA 19104-6340}
\affil[2]{Department of Mathematics\\
University of Pennsylvania\\
David Rittenhouse Laboratories\\ 
209 S. 33rd Street\\
Philadelphia, PA 19104-6395 }
\affil[3]{Department of Computer and Information Science\\
University of Pennsylvania\\
504 Levine, 200 S. 33rd Street\\
Philadelphia, PA 19104-6309}
\date{\vspace{-10ex}}


\begin{document}
\maketitle
\pagestyle{myheadings}
\markboth{Understanding Probability Extremizing}{Satop\"a\"a et al.}
\begin{abstract}
\noindent
\textbf{Summary.} Randomness in scientific estimation is generally 
assumed to arise from unmeasured or uncontrolled factors. However, 
when combining subjective probability estimates, heterogeneity
stemming from people's cognitive or information diversity is often
more important than measurement noise.  This paper presents a novel
framework that models the heterogeneity arising from forecasters that use 
partially overlapping information sources, and applies that model to 
the task of aggregating the probabilities given by a group of forecasters 
who forecast whether an event will occur or not. Our model describes 
the distribution of information across forecasters in terms of easily
interpretable parameters and shows how the optimal amount
of \textit{extremizing} of the average probability forecast (shifting
it closer to its nearest extreme) varies as a function of the forecasters'
information overlap.  Our model thus gives a more principled
understanding of the historically {\it ad hoc} practice of extremizing
average forecasts.\\
\\
\textit{Keywords:} Expert Beliefs; Gaussian Process; Information Aggregation and Diversity; Model
Averaging; Probability Forecasting; Wisdom of Crowds
\end{abstract}

\section{Introduction and Overview}

\subsection{The Forecast Aggregation Problem}
Event forecasting is the science of giving probability estimates for future events.  The classical examples can be found in meteorology
\citep{sanders1963subjective}, where the scientists are often
interested in accurate probability predictions of different weather
phenomena such as rain in the next few days.  The practice, however, has now
spread to many other fields as well. This includes medical diagnosis
and prognosis \citep{wilson1998prediction,pepe2003statistical,
o2006uncertain}, estimation of credit default
\citep{kramer2006evaluating}, and predicting political and
socio-economic events \citep{tetlock2005expert} such as who will win
the next Congolese election, or whether war will break out in Egypt
this year. Accurate probability forecasts on such events are important
for the decision-maker who wants to decide, for instance, what to pack
for tomorrow's family outing or whether to advice against all travel
to Egypt.

Often the decision-maker has access to several different
forecasts on the same event. For instance, one forecast might depend on news broadcasting in the United States while the other
is based on local news in Egypt. Many investigators have shown that combining the individual probabilities into a single consensus forecast typically improves predictive performance and hence improves decision making \citep{clemen1989combining,
armstrong2001combining}. Unfortunately, there are many different ways to combine the forecasts, and the choice of the combination
rule can have a large impact on the predictive quality of the final
aggregate forecast.  
%Analyzing and developing improved rules is the main motivation of 
This motivates \textit{forecast aggregation} that
is the problem of combining multiple forecasts into a single
forecast with optimal properties.

There are two general approaches to forecast aggregation: empirical
and theoretical.  The empirical approach is akin to machine
learning. Given a training set with multiple forecasts on events with
known outcomes, the decision-maker experiments with different
aggregation techniques and chooses the one (and potentially any
parameter values) that yield the best performance on the training set.
The theoretical approach, on the other hand, first constructs a
probability model and then computes the optimal aggregation procedure
under the assumptions of the model.  This may involve estimating model
parameters from the forecasts.

Both approaches are important.  Procedures based on theory that do not
perform well in practice are ultimately of limited use.  On the other
hand, an empirical approach without theoretical underpinnings lack
both credibility (why should we believe it?)  and guidance (in which
direction can we look for improvement?).  As we will discuss below,
the history of forecast aggregation to date is largely
empirical\footnote{Not only the approaches in
Section~\ref{ss:empirical} but also the approaches described in
Section~\ref{ss:measurement}, to which we have provided theoretical
underpinnings, have been pursued in a manner that is largely
empirical.}.  The main contribution of this paper is to introduce a plausible
theoretical framework for forecast aggregation.  This framework allows
us to interpret existing aggregation procedures and illuminate aspects
that can be improved. For instance, the
framework is used to shed light on the practice of {\em probability
extremizing}, i.e. shifting an average aggregate closer to its nearest
extreme. This is an empirical technique that has been widely used to
improve the predictive performance of many simple aggregators such as
the average probability. Finally, the framework is applied to a
specific model under which the optimal aggregator can be computed.

\subsection{Bias, Noise and Forecast Assessment}
\label{BiasNoise}
For the sake of theoretical development, it is convenient to represent the event to be forecasted with an
indicator function that equals one or zero depending whether the event
happens or not, respectively. Each probability forecast is then an
estimator of this indicator function.  Therefore, as is the case with
all estimators, their deviation from the truth can be broken into two
pieces: bias and noise.  In the forecasting literature, a forecaster
is said to be {\em calibrated} if, in the long run, events associated
with a forecast $p \in [0,1]$ have an empirical frequency of
occurrence of $p$ (see, e.g., \citealt{degroot1983comparison}). For
instance, consider all events that the forecaster believes to occur
with probability 0.6. If the forecaster is calibrated, 60\% of
these events will actually end up occurring. The same, of course, has
to be true across all probabilities -- not just 0.6.  
In the lingo of
estimation, this is equivalent to being conditionally unbiased.

Non-expert populations are typically not calibrated. There are,
however, many subpopulations of experts that are highly
calibrated. For instance, meteorologists performing probabilistic
weather forecasting have been found to be calibrated
(\citealt{murphy1977reliability}); experienced tournament bridge
players were highly calibrated in predicting whether a contract would
be made (\citealt{keren1987facing}); and bookmakers generally give
calibrated forecasts on sports events (\citealt{dowie1976efficiency,
yates1985conditional}). If the decision-maker is left with poorly
calibrated forecasts, they can, in principle, be corrected by
translating the forecast $p$ into the forecast $\pt$ where $\pt$ is
the historical proportion of times the event occurs when $p$ is
forecast.  One can then assume without loss of generality that the
forecaster is calibrated, replacing $p$ if necessary by the
historically corrected $\pt$. This assumption is, of course, valid
only in principle: it relies on a relatively lengthy history and an
assumption of stationarity of the properties of the forecasts.  
% For other calibration techniques, see, for instance, \citealt{foster1998asymptotic, Brier}.  
Nevertheless, on the
theoretical level, it is important to separate the problems of bias
and noise because they are solved by different mechanisms.  This
paper considers the aggregation problem for calibrated forecasts,
therefore isolating the task to one of noise reduction.

Before delving into the problem of forecast aggregation, it is useful
to briefly discuss the problem of assessing and improving a single
forecast stream.  The most common methodology is to use a loss
function.  Letting $\one_A$ denote the indicator function of the event
$A$ being forecast, the loss function is a penalty $L(p , \one_A)$
where $p$ is the forecasted probability.  A loss function is said to
be {\em revealing} or {\em proper} if the Bayesian optimal strategy is
to tell the truth.  In other words, if the subjective probability
estimate is $p$, then $t = p$ should minimize the expected loss $p
L(t,1) + (1-p) L(t,0)$.  This expected loss is a one dimensional
assessment of an estimator. Many choices, however, are possible, and
an estimator that outperforms another under one loss function will not
necessarily do so under a different loss function.  For example,
minimizing the quadratic loss function $(p - \one_A)^2$, also known as
the {\em Brier score}, gives the estimator with the least variance. On
the other hand, minimizing the entropy loss $\log (1 / |p -
\one_{A^c}|)$ reveals the estimator that best avoids forecasting an
incorrect event with near certainty (see, e.g., \citealt[Section~2]{HwPe1997}
for a discussion of proper loss functions).

The point of this brief review is two-fold.  First, when a group of
sophisticated forecasters operates under a proper loss function,
the assumption of calibrated forecasts is, to some degree,
self-fulfilling.  Forecasters can improve their long run performance
by calibrating their forecasts via training and active
feedback (\citealt{o2006uncertain}). Forecasters with competitive
motivation, sophistication, and historical data will probably do so. Secondly, the
assessment of the aggregated forecast is not uni-dimensional any more
than is the assessment of an individual forecast.  In order to compare
different aggregation procedures, a scoring rule must be chosen.  In
practice, the Brier score is the most common choice, perhaps because
of its simplicity, its convenient decomposition into three
interpretable parts (See Section \ref{realData}), or because of the
paramount status of variance in the statistical literature. This paper concentrates on minimizing the variance of the aggregators, though
 much of the discussion holds under general proper loss
functions.

\subsection{The Partial Information Framework}
\label{PIFintro}
This section introduces the partial information framework at the most general level. Similarly to any probability model for forecast aggregation, the construction begins with
 a probability space $(\Omega, \F , \P)$ and a
measurable event $A \in \F$ to be forecasted.  In any Bayesian setup,
with a proper loss function, it is more or less tautological that a
forecaster reports $p = \E (\one_A \| \G)$ where $\G \subseteq \F$ is
the \textit{information set} used by the forecaster.  The
\textit{partial information framework} extends this idea to $N$
forecasters. In particular, it posits the existence of $N$
information sets $\F_i \subseteq \F$ such that Forecaster~$i$
reports $p_i = \E (\one_A \| \F_i)$.

The information sets contain only information actually used by the
forecasters. Therefore, if Forecaster $i$ uses a simple rule, the
information set $\F_i$ may not be the full $\sigma$-field of
information available to the forecaster but rather a smaller
$\sigma$-field corresponding to the information actually used by the
rule.  For example, when forecasting the re-election of the president,  a forecaster obeying the dictum ``it's the economy,
stupid!''  might be assigned a $\sigma$-field containing only economic
indicators.  This is reminiscent of a forecasting algorithm that uses
a similarly restricted subset of information fed to it. Therefore
any heterogeneity in the forecasts stems purely from
\textit{information diversity}.  Any further details must come from
assumptions on the structure of the information sets $\{ \F_i : i = 1,
\dots, N\}$.  %%%In Section~?? we explore in detail a particular such structure.

To illustrate, suppose that the forecasters are asked to predict a sequence of events
$\{ A_1, A_2 , \ldots \}$. The general framework only assumes that
Forecaster~$i$ uses some $\sigma$-field $\F_{ij}$ to make a
probability forecast for event $A_j$.  Further aspects of the problem
are reflected by the structure of the collection $\{ \F_{ij} \}$.  For
example, if the events are sequenced in time, then the information
sets $\{ \F_{ij} : j = 1 , 2, 3, \ldots \}$ are likely to form an
increasing sequence.  If the forecasts are public, it may make sense
to assume that $\F_{ij}$ contains all forecasts up to time $j-1$.
%If, in addition, the forecasts of each individual event $A_j$ are sequenced rather %than synchronous, the set $\F_{ij}$ may contain information about %some forecasts at time $j$ as well.  
This is, of course, only one illustration. The framework itself is quite flexible
yet intuitive. In fact, it is not difficult to imagine a variety
of other forecasting setups and how they translate to
different inter-dependencies among the information sets.


For any such setup, the partial information framework distinguishes two benchmarks for aggregation efficiency.  The first is the {\em oracular} aggregator
$p' := \E (\one_A \| \F')$, where $\F'$ is the $\sigma$-field
generated by the union of the information sets $\{ \F_i : i = 1, \dots,
N\}$. Given that it is not possible to improve beyond using all the information available
to the forecasters, the oracular aggregator represents the theoretical optimum and is therefore a
reasonable upper bound on estimation efficiency. Note that if a forecaster chooses to discard
some of the available information, then, for the purposes of aggregation,
that information may as well not exist. Any
information that is not in $\F'$ represents randomness inherent in the
outcome.

In practice, however, information comes to the
aggregator only via the forecasts $\{p_i : i = 1, \dots, N\}$. Given that $\F'$ generally cannot be constructed from these forecasts alone, no practically feasible aggregator can be expected to perform as well as the oracular aggregator.  Therefore a more achievable benchmark is given by the \textit{revealed} aggregator $p'' := \E (\one_A \|
\F'')$, where $\F''$ is the $\sigma$-field generated (or revealed) by the forecasts
$\{ p_i : i = 1, \dots, N \}$. This benchmark minimizes
the expectation of any proper loss function (\citealt{Ranjan08}), and,
unlike the oracular aggregator, can be applied in practice. For this
reason, the revealed aggregator is the relevant estimator in each specific partial information model.

%The {\em general partial information model} 
%is precisely the model described in the foregoing paragaphs.  
%Specific partial information models involve assumptions on
%the structure of $\{ \F_i \}$.  In each case, the relevant estimator 
%is the revealed aggregator. 

\subsection{Organization of the Paper}

The next section reviews some prior work on forecast aggregation and compares it to the partial information framework.
Section~\ref{sec:model} discusses illuminating examples and
motivates our subsequent choices of specific partial information
models.  Section~\ref{extremizing} compares the oracular aggregator with the average probit score, thereby explaining the
empirical practice of probability extremizing.
Section~\ref{aggregation} derives the revealed aggregator for a
particular partial information model and evaluates it on real-world
forecasting data.  The final section concludes with a discussion of
future research.



\section{Prior Work on Aggregation}
\label{sec:prior}
Prior work on forecast aggregation has focused largely on two frameworks that make different assumptions about the source of forecast heterogeneity.  These approaches are discussed in the following
subsections.

\subsection{The Interpreted Signal Framework}
\label{ss:inerpreted}

\citet{hong2009interpreted} introduce the {\em interpreted signal
framework} in which the forecaster's prediction is based on a personal
interpretation of (a subset of) the factors or cues that influence the
future event to be predicted.  Differences among the 
predictions are ascribed to differing interpretation procedures.  For
example, if two forecasters follow the same political campaign speech,
one forecaster may focus on the content of the speech while the other may
concentrate largely on the audience interaction.  Even though the
forecasters receive the same information, they interpret it
differently and therefore are likely to arrive at different forecasts for the probability of the candidate winning the election.

Therefore, under this framework, forecast heterogeneity stems from ``cognitive
diversity''.  This is a very reasonable assumption that has been analyzed and
utilized in many other settings.  For example,
~\citet{parunak2013characterizing} demonstrate that optimal
aggregation of interpreted forecasts is not constrained to the
convex hull of the forecasts; \citet{broomell2009experts} analyze
inter-forecaster correlation under the assumption that the cues can be
mapped to the individual forecasts via different linear regression
functions; \cite{degroot1991optimal} assume that each forecaster first observes a random quantity $X_i$ and then reports $\P(A | X_i)$, 
where $\boldsymbol{X} = (X_1, \dots, X_N)$ has density $g(\boldsymbol{X} | A)$. 
They first derive the optimal aggregator when $X_i$ are conditionally independent and then proceed to determine the optimal weights for a linear opinion pool when $X_i$ are not conditionally independent. However, \cite{Ranjan08} prove that any (non-trivially) weighted average of calibrated forecasts is necessarily uncalibrated and too close to the naive baseline forecast (typically at $0.5$).

%Some preliminary steps have been made towards a more concrete model by, for example, assuming
%conditional independence of the $x_i$, \cite{}, but
%lacking a distributional model. 

To the best of our knowledge, no previous study, however, has provided a general framework for linking the quantity of interest with the interpreted forecasts. Therefore the interpreted signal framework, as proposed, has remained relatively abstract. In response, our partial information framework formalizes the intuition behind it, allows quantitative predictions, and provides a flexible construction that can be adapted to a broad range of forecasting setups. 

%
%The interpreted signal framework, when formalized, more or less
%implies the partial information framework.  Unfortunately, previous
%work on interpreted forecasts has produced only abstract concepts and
%has not been taken to the level of a precise formal model with
%quantitative predictions.  Given this lack of a quantitative model, in
%practice it is often easier and more common to explain subjective
%forecasts with the {\em measurement error framework}. This framework
%is described in the next subsection.

\subsection{The Measurement Error Framework}
\label{ss:measurement}
In the absence of a quantitative
interpreted signal model, prior applications have typically explained forecast heterogeneity  with standard statistical models. These models are different formalizations of  the \textit{measurement error framework} that generates forecast heterogeneity purely from a probability distribution. More specifically, the framework assumes a ``true'' probability
$\theta$  for the future event to be predicted. This probability, which can be interpreted as the  forecast made by an ideal forecaster, is then measured by the individual forecasters with mean-zero idiosyncratic error.  This is typically
formalized by modeling each forecast as an independent draw from a common
probability distribution that is centered at $\theta$. Therefore the underlying probability model is just the 
classical statistical model for measurement with i.i.d. mean zero error. 
Consequently, the average probability,
$$\pb := \frac{1}{N}\sum_{i=1}^N
p_i,$$
is unbiased for $\theta$ and has the least
variance.

Different versions of this approach alter the quantity that is supposedly
measured without bias. If instead, for example, the true log-odds
$\log\left\{\theta/(1-\theta)\right\}$ have been measured without
bias, the average log-odds will be an unbiased estimator. Transforming
this average back into an estimate for $\theta$ yields the {\em
logarithmic opinion pool}
$$\plog := \frac{\exp \left [ \frac{1}{N} \sum_{i=1}^N
   \log \left\{ p_i / (1-p_i) \right\} \right ]} {1 + \exp \left [
\frac{1}{N} \sum_{i=1}^N \log \left\{p_i / (1-p_i)\right\} \right
]} $$ This aggregator has been analyzed and utilized by many
investigators (see, e.g., \citealt{dawid1995coherent, Genest,
bacharach1975group}). Another alternative for the target quantity is
the true probit score $\Phi^{-1}(\theta)$, where $\Phi$ represents the
cumulative distribution function (CDF) of a standard Gaussian
distribution. This results in the {\em probit opinion pool}
$$\probit := \Phi \left\{ \frac{1}{N} \sum_{i=1}^N \Phi^{-1}
   (p_i) \right \} $$

% In this case, the underlying probability model assumes that the forecasters are able
%to observe  the true probit score with i.i.d. mean zero error.
%%%The probit transformation is favored in economics, while other disciplines
%%%more commonly  use the log-odds transformation~(\citet{bryan2013regression}).
%%%The choice is usually made based on computational convenience
%%%versus ease of interpretation: the log-odds are considered more
%%%interpretable, while probit can be more convienient if reported
%%%forecasts are already standardized.
These aggregators illustrate the main advantage of the measurement
error framework: simplicity.  The formula for a new aggregator is
obtained in a straightforward manner by transforming, averaging, and
transforming back.  The underlying probability model is also simple
and very familiar.  There are, however, a number of disadvantages.
First, these \textit{averaging aggregators} target the mean of
(potentially transformed) probability forecasts. The correct target,
however, is the event indicator $\one_A$ that lies on the boundary of
the sample space. Important properties, such as calibration, cannot be
expected from an aggregator that is based on a model that not does link the observations and the correct quantity
of interest.

Second, the introduction of a nonlinear transformation biases the
estimator.  Therefore neither $\plog$ nor $\probit$ produces a
calibrated forecast. Exactly how problematic this is depends on
whether the bias is very small relative to noise or other
adjustments.  The direction of the bias can sometimes be
determined. For instance, \citet{Ranjan08} prove that any convex
combination of calibrated forecasts will be under-confident, that is,
too close to $0.5$. This result applies to the averaging aggregators
because they are by nature different convex combinations of the
forecasts.

A third and more serious disadvantage is the implausibility of the
underlying model. Relying on a true probability $\theta$ is vulnerable to
many philosophical debates, and even if one eventually manages to
convince one's self of the existence of such a quantity, it is
difficult to believe that the forecasters are actually seeing the
hidden parameter $\theta$ (or $\log\{\theta/(1-\theta)\}$ or
$\Phi^{-1}(\theta)$) with independent noise. Therefore, whereas the
interpreted signal framework proposes a micro-level explanation, the
measurement error model does not; at best, it forces us to imagine that
the forecasters are all in principle trying to apply the same
procedures to the same data but are making numerous small mistakes.

A fourth disadvantage is that the averaging aggregators do not often perform
very well in practice. For one thing, any transformed average is constrained to
the convex hull of the individual forecasts, hence contradicting the
interpreted signal framework (\citealt{parunak2013characterizing}).  Empirically, one sees
for many data sets that an aggregator restricted to the convex hull
will be far from optimal. \cite{hong2009interpreted} also demonstrate that the standard
assumption of conditional independence induces a specific and highly
unrealistic structure on interpreted forecasts.

\subsection{Empirical Approaches}
\label{ss:empirical}

If one is not concerned with theoretical justification, an obvious
approach is to perturb one of these estimators and observe whether the
adjusted estimator performs better on some data set of interest.  It
is known both theoretically and empirically that the measurement error
framework produces under-confident aggregators.  A popular adjustment,
therefore, is to {\em extremize}, that is, to apply a
post-transformation that shifts the aggregated forecast closer to the
nearer extreme (either zero or one).  For instance,~\citet{Ranjan08}
propose transforming the (potentially weighted) $\pb$ with the CDF of a
beta distribution.  If both the shape and scale of this beta
distribution are equal and constrained to be at least~1.0, the
aggregator extremizes and has some attractive theoretical
properties~\citep{Wallsten2001}.  ~\citet{satopaa} use a logistic
regression model to derive an aggregator that extremizes the average
log-odds.  ~\citet{baron2014two} give two intuitive justifications for
extremizing and discuss an extremizing technique that has previously
been used by a number of investigators~\citep{Erev1994,
shlomi2010subjective}; one could perhaps trace this all the way back
to~\citet{karmarkar1978subjectively}.  In an empirical
study,~\citet{mellers2014psychological} show that extremizing can
improve aggregate forecasts of international events.
%%%\citet{turner2013forecast} and \citet{Ariely00theeffects} 
%%%also mention the benefits of extremizing.

These and many other studies represent the unwieldy position of the
 current state-of-the-art aggregators: they first compute an average 
 based on a model that is
likely to be at odds with the actual process of probability
forecasting, and  then aim to correct the induced bias  via {\em ad hoc}
extremizing techniques.
%
Not only does this leave something to be desired from an explanatory
point of view, these approaches are also subject to the problems of
machine learning, such as overfitting.  For example, the amount of 
extremizing is typically learned by optimizing a
scoring rule over a separate training set
(see~\citealt{Gneiting04strictlyproper} for a discussion on scoring
rules).  This requires repeated realizations
of a single event and therefore cannot be applied to a single event with an unknown outcome. Overall, while {\em ad hoc}
extremization works to some degree, such techniques provide little
insight beyond the amount of extremizing itself. Most importantly, they do not point
the way to continued improvement.

The present paper aims to remedy this situation.  The framework of
partial information produces theoretically based estimators which
explain extremization. Under a simple instance of the partial
information framework, the amount of extremization can be quantified
with a single number that follows a Cauchy distribution.  Given that
this distribution depends on the structure of information among the
forecasters, it establishes a link between the information structure
and the amount of necessary extremizing.  This leads to three main results:  First, the modal
amount of extremizing is increasing in the level of information
diversity and the total amount of information among forecasters.
Second, no matter how information is distributed among the forecasters,
extremizing the probit opinion pool is more likely to be beneficial
than harmful (except in the degenerate situation where all forecasts
are the same). Third, the structure of the information overlap establishes a spectrum of aggregators, ranging from
average- (most information is public) to voting-like (most information
is private) techniques.


\section{The Gaussian Partial Information Model}
\label{sec:model}

\subsection{Motivating Examples}

Partial information models are sensitive to the structure of the
information overlap that is assumed to hold among the individual
forecasters. Model sensitivity is useful if it reacts
to important inherent features of the problem, but harmful if it adds
noise that is not reflected in the actual best response to the data.
It therefore behooves us to begin with some simple examples to show
that the optimal aggregate forecast is not well defined without
assumptions on the information structure among the forecasters.

\begin{example}
\label{FirstExample}
Suppose two forecasters both report a probability of $2/3$ for some
event.  If they are seeing the same evidence, then the optimal aggregate
forecast is $2/3$.  If they are seeing different evidence,
then clearly the combined evidence should give an aggregate forecast
somewhat greater than $2/3$.  To make the situation more concrete, consider a basket containing a fair coin and a two-headed coin. The forecasters are asked to predict whether a coin chosen at random is in fact
two-headed. Before making their predictions, each forecaster observes the result of a single flip of
the chosen coin.  If they see HEADS, then the correct
Bayesian probability estimate is $2/3$.  If both forecasters see the
result of the same coin flip, then the optimal aggregate forecast
 is again $2/3$. On the other hand, if they observe different (presumably
independent) coin flips, then the optimal aggregate forecast 
is $4/5$.
\end{example}
In this example, it is not possible to distinguish between the two different information structures simply based on the given predictions. If the forecasters had participated in a long sequence of independent replications of the given task, it may have been possible to eventually ascertain whether they have the same information or not. Similarly, if their forecasts had disagreed, it would have been easy to conclude that they did not observe the same coin flip.  However, in the given situation, there
was no way to know, and neither $2/3$ nor $4/5$ can be said to be a
better choice for the aggregate forecast.  Therefore, we conclude that it is necessary to incorporate an assumption as to the structure of the information
overlap, and that the details must be informed by the particular
instance of the problem. The next example shows that even if the forecasters observe independent events, further details
in the structure of information can still greatly affect the
optimal aggregate forecast.

\begin{example}
Let $\Omega = \{ A,B,C,D \} \times \{ 0,1 \}$ be a probability space
with eight points.  Consider a measure $\mu$ that assigns
probabilities $\mu (A,1) = a/4, \mu (A,0) = (1-a)/4$, $\mu (B,1) =
b/4, \mu (B,0) = (1-b)/4$, and so forth. Define two events
\begin{align*}
 S_1 &= \{A,0),(A,1),(B,0),(B,1) \}\\
 S_2 &= \{A,0),(A,1),(C,0),(C,1) \}
\end{align*}
In other words, $S_1$ is the event that the first coordinate is
$A$ or $B$, and $S_2$ is the event that the first coordinate
is $A$ or $C$. Suppose that Forecasters $1$ and $2$ observe $S_1$
and $S_2$, respectively. Therefore the $i$th Forecaster's information set is
given by the $\sigma$-field $\F_i$ containing $S_i$ and its
complement. Given that $S_1$ and $S_2$ are independent, the $\sigma$-fields $\F_1$ and $\F_2$ are also independent. Now, let
$G$ be the event that the second coordinate is~1.  Forecaster~1
reports $p_1 = \P(G | \mathcal{F}_1) = (a+b)/2$ if $S_1$ occurs;
otherwise, $p_1 = (c+d)/2$.  Forecaster~2, on the other hand, reports
$p_2 = \P(G | \mathcal{F}_2) = (a+c)/2$ if $S_2$ occurs; otherwise,
$p_2 = (b+d)/2$.  If $\ee$ is added to $a$ and $d$ but subtracted from
$b$ and $c$, the forecasts $p_1$ and $p_2$ do not change, nor does it
change the fact that each of the four possible pairs of forecasts has
probability $1/4$.  Therefore all observables are invariant under
this perturbation.  If Forecasters $1$ and $2$ report $(a+b)/2$ and
$(a+c)/2$, respectively, then the aggregator knows, by considering the
intersection $S_1 \cap S_2$, that the first coordinate is $A$.
Consequently, the optimal aggregate forecast is $a$, which is most
definitely affected by the perturbation.
\end{example}

The point of this example is to show that the aggregation problem is affected
by the fine structure of information overlap.  It is, however, unlikely
that the structure can ever be known with the precision postulated in
this simple example.  Therefore it is necessary to make reasonable
assumptions that yield plausible yet generic information structures.

\subsection{Gaussian Partial Information Model}
\label{ss:Gaussian}
The general partial information framework can be abstracted by first constructing a pool of information particles. Each particle, which can be interpreted as the smallest unit of information, is either positive or negative. The positive particles provide evidence in favor of the event $A$. 
%For instance, a subset of positive particles may represent a news article with  evidence that $A$ is likely to happen. 
In the contrary, the negative particles provide evidence against $A$. Therefore, if the overall sum (integral) of the positive particles is larger than that of the negative particles', the event $A$ happens; otherwise, it does not. Each forecaster, however, observes only the sum of some subset of the particles. Based on this sum, the forecaster makes a probability estimate for the event $A$. This is made concrete in the following model that  represents the pool of information with the unit interval and generates the information particles from a Gaussian process. 

\begin{quote}
{\bf Model.} Denote the pool of information with the unit interval $S = [0,1]$. Consider a centered Gaussian process $\{X_B\}$ that is defined on a probability space $(\Omega
, \F , \P)$ and indexed by the Borel subsets $B \subseteq S$ such that the
covariances $\Cov (X_B , X_{B'}) = |B \cap B'|$.  In other words, the
unit interval $S$ is endowed with Gaussian white noise, and $X_B$ is the
total of the white noise in the Borel subset $B$.  Let $A$ denote the
event that the sum of all the noise is positive: $A := \{ X_S > 0 \}$.
For each $i = 1, \dots, N$, let $B_i$ be some Borel subset of $S$, and
define the corresponding $\sigma$-field as $\F_i := \sigma (X_{B_i})$. Forecaster $i$ then predicts $p_i :=
\E (\one_A \| \F_i)$.
\end{quote}

This model can be interpreted and motivated by recalling the interpreted signal
model of~\citet{broomell2009experts}, in which the forecasters
use different linear functions of the cues that influence the future event to be predicted.  In other words,
Forecaster~$i$ forms an opinion based on $L_i (Z_1 , \ldots , Z_r)$,
where each $L_i$ is a linear function of some observable quantities
$Z_1 , \ldots , Z_r$.  Proposing a linear model for subjective
interpretation seems quite natural and is in fact the only such
postulate we know to have been suggested for interpreted signals.  If
the observables (or any linear combination of them) are independent
and have small tails, then as $r \to \infty$, the joint distribution
of the linear combinations $L_1 , \ldots , L_N$ will be asymptotically
Gaussian.  Given that the number of observables in a real-world setup is likely to be large, it makes sense
to model the forecasters' observations as jointly Gaussian.  The
following enumeration provides interpretation and clarify which
aspects of the Gaussian model are essential and which have little or
no impact.

\begin{enumerate}[(i)]
\item {\bf Interpretations.} It is not necessary to assume anything 
about the source of the information.  For instance, the information 
could stem from photographs, survey research, records, books, 
interviews, or personal recollections.  All these details have 
been abstracted away.

\item {\bf Information Sets.} The set $B_i$ holds the information used
by Forecaster $i$, and the covariance $\Cov (X_{B_i} , X_{B_j}) = |B_{i} \cap B_{j}|$
represents the information overlap between Forecasters $i$ and
$j$.
%\item In the interpreted signal framework, the set $B_i$ represents 
%the linear regressor used by Forecaster~$i$, and the covariance 
%structure represents degrees of similarities between regressors 
%of different forecasters.  
Consequently, the complement of $B_i$ holds information not used by
Forecaster~$i$.  No assumption is necessary as to whether this
information was unknown to Forecaster~$i$ instead of known but not
used in the forecast.


\item {\bf Pool of Information.} First, the pool of
information potentially available to the forecasters is the white
noise on $S = [0,1]$. The role of the unit interval
is for the convenient specification of the sets $B_i$.
The exact choice is not relevant, and
any other set could have been used. The unit interval, however, is a
natural starting point that provides an alternative interpretation of $|B_j|$ as marginal probabilities for some $N$ events, $|B_i \cap B_j|$ as their pairwise joint probabilities, $|B_i \cap B_j
\cap B_k|$ as their three-way joint probabilities,
and so forth.  This interpretation is particularly useful in analysis as it links the
information structure to many known results in combinatorics and geometry. This paper discusses one such connection in Proposition \ref{CorrelationPolytope} and another in Section \ref{compound}. Second, there is
no sense of time or ranking of information within the
pool. Instead, the pool is a collection of information, where each
piece of information has an {\em a priori} equal chance to contribute
to the final outcome.  Quantitatively, the information is parametrized
by the length, a.k.a. the Lebesgue measure, on $S$.
\label{item:pool}


\item {\bf Invariant Transformations.}  From the empirical point of
view, the exact identities of the individual sets $B_i$ are
irrelevant.  All that matters are the covariances $\Cov \left(X_{B_i}
, X_{B_j}\right) = |B_i \cap B_j|$.  The explicit sets $B_i$ are for
use in analysis.

\item {\bf Scale Invariance.} The model is invariant under rescaling,
replacing $S$ by $[0,\lambda]$ and $B_i$ by $\lambda B_i$.  Therefore,
the actual scale of the model (e.g., the fact that the covariances of
the variables $X_B$ are bounded by $1$) is not relevant.


\item {\bf Specific versus General Model.} A specific model requires a
choice of an event $A$ and Borel sets $B_i$.
This might be done in several ways: a) by choosing them in advance,
according to some criterion; b) estimating the parameters $\P(A)$,
$|B_i|$ and $|B_i \cap B_j|$ from data; c) using a Bayesian model
with a prior distribution on the unknown parameters.  This paper
focuses mostly on a) and b) but discusses c) shortly in
Section~\ref{discussion}.  Section \ref{extremizing} provides one
result, namely Proposition~\ref{positiveProbThm} that holds for any (nonrandom) choices of the sets $B_i$.
\label{item:specific}






\item {\bf Choice of Target Event.}  There is one substantive
assumption in this model, namely the choice of the half-space $\{ X_S
> 0 \}$ for the event $A$.  Changing this event results
in a non-isomorphic model. The current choice
implies a prior probability $\P(A) = 1/2$, which seems as
uninformative as possible and therefore provides a natural starting
point. Note that specifying a prior distribution for $A$ cannot be avoided as
long as the model depends on a probability space. As was mentioned in
Section \ref{PIFintro}, this includes essentially any probability
model for forecast aggregation.
\label{item:choice}

\item {\bf Centering.} One could choose a non-central half-space $\{
X_S > t \}$ with $t \neq 0$ for the event $A$.  This would make the prior
probability of $A$ different from $1/2$.  The current paper focuses on the
centered model for simplicity but also for the following reason.  The
Gaussian assumption is most often true in practice when the
observations are centered.  For example, if the electorate
is broken into a dozen demographic segments, the portion of voters in
each segment that vote for a given candidate does not follow a
Gaussian distribution, but the difference between this portion and the
historically expected or predicted portion may.  A centered partial
information model is then quite realistic when forecasting the event
of a positive deviation.  The political futures website, Intrade,
during its operation, was in fact well stocked with centered events. For instance, the website might have inquired whether 
 the number of representatives elected from a
given party would exceed a threshold that had been adjusted to
bring the prior probability near $1/2$.  Sports betting websites also operate
almost exclusively in this mode.
\label{item:centered}
\end{enumerate}

\subsection{Preliminary Observations}
\label{prelim}
It is helpful to begin the introduction of the Gaussian model by first considering only two forecasters.  Suppose Forecasters $1$ and $2$ observe respective $\delta_1$ and
$\delta_2$ portions of the Gaussian process.  If $\rho$ denotes the
size of the overlap in their information sets, then 
\begin{align*}
|B_1| &= \delta_1\\
|B_2| &= \delta_2\\
|B_1 \cap B_2| &= \rho
\end{align*}
\begin{figure}[t]
%   \hspace{-2em}
   \includegraphics[width = \textwidth]{N=2} % requires the graphicx package
   \caption{Illustration of the Gaussian partial information model with $2$ forecasters.}
   \label{diagram2}
\end{figure}
Figure \ref{diagram2} illustrates this setup.  In this diagram, the
Gaussian process has been partitioned into four parts based on the
forecasters' information sets:
\begin{align*}
 U &= X_{B_1 / B_2}
& M &= X_{B_1 \cap B_2}\\
 V &= X_{B_2 / B_1}
& W &= X_{(B_1 \cup B_2)^c}
\end{align*}
This partition illustrates the additive nature of the information pool. In particular,
\begin{align*}
X_{B_1} &= U + M\\
X_{B_2} &= M + V\\
X_S &= U+M+V+W,
\end{align*}
where $U, V, M, W$ are independent Gaussian random variables with respective variances
$\delta_1-\rho$, $\delta_2-\rho$, $\rho$, and $1+\rho-\delta_1 -
\delta_2$. The joint distribution of $X_{S}$, $X_{B_1}$, and $X_{B_2}$ is a
multivariate Gaussian distribution.  That is,
\begin{align}
\left(\begin{matrix} X_S \\ X_{B_1}\\ X_{B_2} \end{matrix}\right) 
 &\sim \mathcal{N}\left(
 \boldsymbol{0},  \left(\begin{matrix} 
1 & \delta_1 & \delta_2\\
\delta_1 & \delta_1 &\rho\\
\delta_2 & \rho & \delta_2
 \end{matrix}\right)\right) \label{twoforecasters}
\end{align}
Given that $X_S$ has mean zero, the prior probability for the event $A$ is
$\P(X_S > 0) = \P(A) = 1/2$. This can be easily adjusted to any probability
$\tilde{p}$ by letting $A = \{ X_S > \Phi^{-1}(1-\tilde{p}) \}$.  This
paper, however, focuses on the centered model with prior probability
$1/2$ (see Remarks (\ref{item:choice}) and (\ref{item:centered}) above). .

\begin{figure}[t]
   \includegraphics[width = \textwidth]{N=N} % requires the graphicx package
   \caption{Illustration of the Gaussian partial information model with $N$ forecasters.}
   \label{diagramN}
\end{figure}

Consider now $N$ forecasters. Let $|B_i| = \delta_i$ be the amount of
information known by Forecaster $i$ for $i = 1, \dots, N$, and $|B_i
\cap B_j| = \rho_{ij} = \rho_{ji}$ be the information overlap between
Forecasters $i$ and $j$. A possible scenario of this general case is illustrated in Figure \ref{diagramN}. Note that $B_i$ does not have to be a contiguous segment of the unit
interval.  Instead, each forecaster can know any Borel measurable
subset of the full information.  The multivariate Gaussian distribution (\ref{twoforecasters}) generalizes to the vector
$\left(X_{S}, X_{B_1}, X_{B_2}, \dots, X_{B_N}\right)$ as follows.
\begin{align}
\left(\begin{matrix} X_S \\ X_{B_1}\\ \vdots \\ X_{B_N} \end{matrix}\right) &\sim \mathcal{N}\left( \left(\begin{matrix} 
\mu_1 \\ \boldsymbol{\mu}_2
 \end{matrix}\right) =
 \boldsymbol{0}, \left(\begin{matrix} 
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}\\
 \end{matrix}\right) 
 =
 \left(\begin{array}{c | c c cc }
1 & \delta_1 & \delta_2 & \dots & \delta_N  \\ \hline
\delta_1 & \delta_1 &\rho_{1,2} & \dots & \rho_{1,N}   \\ 
\delta_2 & \rho_{2,1} & \delta_2 & \dots & \rho_{2,N}  \\ 
\vdots & \vdots & \vdots & \ddots & \vdots  \\ 
\delta_N & \rho_{N,1} & \rho_{N,2} & \dots & \delta_N\\ 
 \end{array}\right)\right)  \label{Nforecasters}
\end{align}

 Given that the information structure
is described by the sub-matrix $\Sigma_{22}$, learning about the
information among the forecasters is equivalent to estimating a
covariance matrix under several restrictions.  In particular, the
information in matrix $\Sigma_{22}$ must be describable by a diagram
such as Figure \ref{diagramN}. If this is the case,
the matrix $\Sigma_{22}$ is called \textit{coherent}.  Coherence
clearly requires that $0 \leq \rho_{ij} \leq \delta_i \leq 1$ for all $i \neq j$.  These
conditions, however, are not sufficient. The parameters are heavily linked to each other and form a highly constrained space. 
%For instance,  consider three forecasters with $\delta_i =  0.5$ for $i = 1, 2, 3$. If $\rho_{12} = 0.25$, then $0.25 \leq \rho_{31} + \rho_{32} \leq 0.75$. This can be verified easily with a diagram such as Figure \ref{diagramN}. But as $N$ grows large, visual inspections of 
%
%Unfortunately, for a general $N$, the constraints cannot be obtained via triangle inequalities anymore. Instead, under any number of forecasters $N$, the space of coherent information structures can be expressed as a convex hull of $2^N$ points. This description is provided in the following proposition. 
%To develop some intuition of this complexity, it is helpful to illustrate the parameter interdependencies with a simple example. 
%\begin{example}
%Consider three forecasters with $\delta_i =  0.5$ for $i = 1, 2, 3$.  If $\rho_{12} = 0.25$, then Forecasters 1 and 2 observe $|B_1 \cup B_2| = 0.75$ of the full information. This is enough to restrict the remaining parameters $\rho_{13}$ and $\rho_{23}$ beyond $0 \leq \rho_{j3} \leq 0.5$ for $j = 1, 2$. To see this, observe that the sum $\rho_{13} + \rho_{23}$ is maximized when Forecaster 3 knows the same information as, say, Forecaster 1. In this case, $\rho_{13} = 0.5$ and  $\rho_{23} = 0.25$. On the other hand, the sum is minimized if Forecaster 3 knows all the remaining information that is not known by Forecasters 1 and 2, but shares the rest of the information with, say, Forecaster 2; that is, $\rho_{13} = 0$ and $\rho_{23} = 0.25$. Therefore  $0.25 \leq \rho_{31} + \rho_{32} \leq 0.75$. 
%\end{example}
%In this example, simple triangle inequalities were enough to derive the necessary bounds for the parameters. However, as $N$ gets large, the structures that cannot be described with triangle inequalities anymore. 
%Instead, under any number of forecasters $N$, the space of coherent information structures can be expressed as a convex hull of $2^N$ points. This description is provided in the following proposition. 
The following proposition describes the space of coherent information structures as convex hull of $2^N$ vertices.
 The proof of this and other propositions are deferred to the Appendix.

\begin{proposition}
\label{CorrelationPolytope}
The overlap structure $\Sigma_{22}$ is coherent if and only
if 
\begin{align*}
\Sigma_{22} \in \COR(N) &:= \text{conv}\left\{
\boldsymbol{x}\boldsymbol{x}' : \boldsymbol{x} \in
\{0,1\}^N\right\},
%&= \left\{ \sum_{i=1}^{2^N} \lambda_i  \right\}\\
\end{align*}
where $\text{conv}\{\cdot\}$ denotes the convex hull and $\COR(N)$ is known as the correlation
polytope or the boolean quadric polytope. The correlation polytope is described by $2^N$
vertices in dimension $\text{dim}(\COR(N)) = \binom{N+1}{2}$.
\end{proposition}
The correlation polytope is known to have a very complex
$\mathcal{H}$-representation, i.e. a description in terms of a finite
number of half-spaces (\citealt{padberg1989boolean, ziegler2000lectures}). Complete descriptions of the facets of $\COR(N)$ are known for $N \leq 7$ and conjectured for  $\COR(8)$ and $\COR(9)$ (\citealt{ziegler2000lectures, christofsmapo, pitowsky1991correlation}). For instance, $\COR(2)$ is described with
\begin{align*}
\begin{cases}
 0 \leq \rho_{12} \leq \min(\delta_1, \delta_2)\\
 \delta_1 + \delta_2 - \rho_{12} \leq 1
\end{cases}
\end{align*}
and $\COR(3)$ with 
\begin{align*}
\hspace{5em}\begin{cases}
 0 \leq \rho_{ij} \leq \min(\delta_i, \delta_j)\\
 \delta_i + \delta_j - \rho_{ij} \leq 1\\
 \delta_i - \rho_{ij} - \rho_{ik} + \rho_{jk} \geq 0\\
 \delta_1 + \delta_2 + \delta_3 - \rho_{12} - \rho_{13} - \rho_{23} \leq 1
\end{cases}
\end{align*}
for $i,j,k \in \{1,2,3\}$ such that $i \neq j$, $i \neq k$, and $j \neq k$. After this the $\mathcal{H}$-representation of $\COR(N)$ becomes increasingly complex. For instance, $\COR(5)$ is known to have 56 facets while $\COR(9)$ has at least 12,246,651,158,320 facets. In fact, because the unconstrained quadratic
0-1 problem is NP-hard, it is probably hopeless to find a complete
list of linear inequalities to describe $\COR(N)$ under general $N$
(\citealt{deza1997geometry}). Fortunately, previous literature has introduced both linear and semidefinite relaxations of $\COR(N)$ (see, e.g., \citealt{laurent1997connections}). Such relaxations together with modern optimization techniques and sufficient data allow the general information structure to be estimated efficiently. This, however, is not in the scope of this paper and is therefore left for subsequent work. 


The next step is
to link the pool of information with the probability forecasts. Given that $X_S | X_{B_i} \sim \mathcal{N}\left(X_{B_i}, 1-\delta_j\right)$, the probability estimate reported by Forecaster $i$ is
\begin{align*}
p_i &= \P\left(A | \mathcal{F}_{i}\right)\\
 &= \P\left(X_S > 0 | X_{B_i}\right)\\
 &= \Phi\left( \frac{X_{B_i}}{\sqrt{1-\delta_i}}\right) 
\end{align*}
This forecast is a conditional expectation of $\one_A$ given $\mathcal{F}_i$ and hence  calibrated. Its marginal distribution can be computed by first denoting the probit score with $P_{i} := \Phi^{-1}(p_i) = X_{B_i}/\sqrt{1-\delta_i}$. The Jacobian for the map $P_{i} \to \phi(P_i)$ is
\begin{eqnarray*}
J(P_i) &=& (2\pi)^{-1/2} \exp \left( - P_i^2/2   \right) 
\end{eqnarray*}
If $h(P_i)$ denotes the Gaussian density of $P_i \sim \mathcal{N}\left(0, \delta_i / (1-\delta_i)\right)$,
the marginal density for $p_i$ is
\begin{align*}
 m\left(p_i | \delta_i \right) &= h(P_i) J(P_i)^{-1} \bigg|_{P_i = \Phi^{-1}(p_i)}\\
&= \sqrt{\frac{1-\delta_i}{\delta_i}} \exp 
   \left\{ \Phi^{-1}(p_i)^2 \left(1-\frac{1}{2 \delta_i} \right) \right\} 
\end{align*}
This distribution has very intuitive behavior. In particular, it is uniform on $[0,1]$ if the forecaster knows half of the
information, i.e. $\delta_i = 1/2$.  It is
unimodal with a minimum at $p_i = 1/2$ when $\delta_i > 1/2$ and a
maximum at $p_i = 1/2$ when $\delta_i < 1/2$.  As $\delta_i \to 0$ (no
information), $p_i$ converges to a point mass at $1/2$
(non-informative forecast) and as $\delta_i \to 1$, $p_i$ converges to
a correct forecast, whose distribution has atoms of weight $1/2$ at
zero and one. Therefore a forecaster with no information ``withdraws'' from the problem by reporting $0.5$ while a forecaster with full information always predicts the correct outcome. Figure \ref{marginals} illustrates the marginal
distribution when $\delta_i$ is equal to $0.3$, $0.5$, and $0.7$.

\begin{figure}[t]
\centering
	\hspace{0em}\includegraphics{LegendMarginal}

 \includegraphics[width= 0.55\textwidth]{Marginals}
   \caption{The marginal distribution of $p_i$ under different levels of 
$\delta_i$.  The more the forecaster knows, i.e. the higher $\delta_i$ is, 
the more the probability forecasts are concentrated around the extreme 
points 0 and~1.}
\label{marginals}
\end{figure}

\section{Probability Extremizing}
\label{extremizing}
This section first introduces the oracular aggregator $p'$ under the
Gaussian model and then uses it to study
extremizing of the probit opinion pool $\probit$. The probit opinion 
pool  was chosen for the analysis because a) it is arguably more
reasonable than the simple average $\bar{p}$; and b) it is very
similar to the logarithmic opinion pool $\plog$ but results in cleaner
analytic expressions. Even though the results are mainly targeted at forecasting practitioners, the discussion serves as an illustration on how the oracular aggregator can be used to benchmark and understand other aggregation techniques. 

\subsection{General Information Structure}
%We show that, under two specific Gaussian partial 
%information models, the oracle, on average,
%extremizes the probit aggregator. 
Recall from Section \ref{PIFintro} that the oracular aggregator is the
conditional expectation of $\one_A$ given all the information known to the
forecasters. Under the Gaussian model, this can be
emulated with an oracle forecaster whose information set is
$B' := \bigcup_{i=1}^N B_i$.  Appending this to the multivariate
Gaussian distribution~(\ref{Nforecasters}) gives
\begin{align}
\left(\begin{matrix} X_S \\ X_{B'} \\ X_{B_1}\\ \vdots \\ X_{B_N} 
 \end{matrix}\right) &\sim \mathcal{N}\left( 
 \boldsymbol{0}, 
% \left(\begin{matrix} 
%\Sigma_{11}' & \Sigma_{12}'\\
%\Sigma_{21}' & \Sigma_{22}\\
% \end{matrix}\right) 
% =
% 
 \left(\begin{array}{c c| c c cc }
1 & \delta' & \delta_1 & \delta_2 & \dots & \delta_N  \\ 
\delta' & \delta' & \delta_1 & \delta_2 & \dots & \delta_N  \\ \hline
\delta_1& \delta_1 & \delta_1 &\rho_{1,2} & \dots & \rho_{1,N}   \\ 
\delta_2 & \delta_2 &\rho_{2,1} & \delta_2 & \dots & \rho_{2,N}  \\ 
\vdots &\vdots & \vdots & \vdots & \ddots & \vdots  \\ 
\delta_N &\delta_N & \rho_{N,1} & \rho_{N,2} & \dots & \delta_N\\ 
 \end{array}\right)\right), \label{oracleN}
\end{align}
where $X_{B'}$ is the information known to the oracle and $\delta' =
|B'|$.  Therefore the oracular aggregator under the Gaussian model is
 \begin{align*}
p' = \P(X_S > 0 |  \mathcal{F}') 
   &= \Phi\left( \frac{X_{B'}}{\sqrt{1-\delta'}} \right)
\end{align*}

Given that no aggregator can improve upon $p'$, it provides a
benchmark for quantifying how much extremizing should be performed
under any given information structure. This can be made more specific by first recalling that a
probability $p$ is extremized by another probability $q$ if and only
if $q$ is closer to $0$ when $p \leq 0.5$ and closer to $1$ when $p
\geq 0.5$. Equivalently, $q$ extremizes $p$ if and only if its probit score $\Phi^{-1}(q)$ is on the same side but further away from zero than $\Phi^{-1}(p)$. The amount of (multiplicative) extremization can then be quantified with the {\em probit extremization ratio} defined as the ratio
 $\alpha(q,p) := \Phi^{-1}(q) / \Phi^{-1} (p)$. This section focuses on the special case
\begin{align}
\alpha(p', \probit)  = \frac{P'}{\frac{1}{N}\sum_{i=1}^N P_{i}}, \label{alpha}
\end{align}
where $P' = \Phi^{-1}(p')$. From now on, unless otherwise stated, expression (\ref{alpha}) is referred simply with
$\alpha$. 
Under this definition, $\probit$
requires extremization if and only if $\alpha > 1$, and the larger $\alpha$ is, the more $\probit$ should be extremized. Note that $\alpha$ a random quantity that spans the entire real line;
that is, it is possible to find a set of probability forecasts and a
distribution of information for any possible value of $\alpha \in
\mathbb{R}$.  Evidently, extremizing is not guaranteed to always
improve the probit opinion pool.  To understand when extremizing is
likely to be beneficial, it is necessary to derive the probability
distribution of $\alpha$.  First, given that
\begin{align*}
P' &\sim \mathcal{N}\left(0, \sigma^2_{1} = 
  \frac{\delta'}{1-\delta'} \right)\\ \frac{1}{N}\sum_{i=1}^N P_{i} 
&\sim \mathcal{N}\left(0, \sigma^2_{2} =\frac{1}{N^2} 
  \left\{ \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} 
  + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}
  \right\} \right),
\end{align*}
the amount of extremizing $\alpha$ is a ratio of two correlated
Gaussian random variables.  The Pearson product-moment correlation
coefficient for them is
\begin{align*}
\kappa  &= 
  \frac{ \sum_{i=1}^N \frac{\delta_i}{\sqrt{1-\delta_i}}}
  {\sqrt{\delta'  \left\{ \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} + 2 
  \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}\right\}}}
  \; 
\end{align*}
It follows that $\alpha$ has a Cauchy distribution as long as
$\sigma_1 \neq 1$, $\sigma_2 \neq 1$, or $\kappa \pm 1$ (see, e.g.,
\citealt{cedilnik2004distribution}).  These
conditions are very mild under the Gaussian model.
For instance, if no forecaster knows as much as the oracle, the
conditions are satisfied.  Consequently, the probability density
function of $\alpha$ is
\begin{align*}
f(\alpha | x_0, \gamma) &= \frac{1}{\pi} 
  \frac{\gamma}{(\alpha-x_0)^2+\gamma^2}, 
\end{align*}
where 
\begin{align*}
x_0 = \kappa \frac{\sigma_1}{\sigma_2} \hspace{0.2in} \mbox{ and } 
  \hspace{0.2in} \gamma &= \frac{\sigma_1}{\sigma_2} \sqrt{1-\kappa^2} \, .
\end{align*}
The parameter $x_0$ represents the location (the median and mode) and
$\gamma$ specifies the scale (half the interquartile range) of the
Cauchy distribution. This leads to the following proposition.  

\begin{proposition}
\label{positiveProbThm}
The law of the extremization ratio $\alpha$ is a Cauchy with
parameters $x_0$ and $\gamma$, where the location parameter $x_0$ is
at least~1, equality occurring only when $\delta_i = \delta'$ for all
$i = 1, \dots, N$. Consequently, if $\delta_i \neq \delta'$ for some
$i = 1, \dots, N$, then the probability that the probit opinion pool $\probit$
requires extremizing is $\P(\alpha > 1 | \Sigma_{22}, \delta')$ which
is strictly greater than $1/2$.
\end{proposition}
\noindent
This proposition shows that, on any non-trivial problem, a small
perturbation in the direction of extremizing is more likely to improve
the probit opinion pool than to degrade it.  This partially explains
why extremizing aggregators perform well on large sets of real-world
prediction problems.  It may be unsurprising after the fact, but the
forecasting literature is still full of articles that perform
probability averaging without extremizing.  The next two sections
examine sub-cases in which more detailed computations of the oracular
aggregator can be performed. This sheds light on extremization in many
real-world forecasting setups.

\subsection{Fully- and Non-Overlapping Information}
\label{disjoint}
This section examines two limiting cases: a) the forecasters share
no information; and b) the forecasters share all their
information. In both cases the revealed aggregator coincides with the oracular aggregator. First, however, consider the latter case where all the information
sets are the same, i.e. $B_{i} = B_j$ for all $i \neq j$. Consequently, all
the probability forecasts $\{p_i : i = 1, \dots, N\}$ are the same,
and the probit opinion pool, the oracular aggregator, and the revealed
aggregator equal to this common probability forecast. Therefore no
extremization is needed. Given that the oracular forecast varies
smoothly over the space of information structures, averaging
techniques, such as the probit opinion pool, can be expected to work
well when the forecasts are based on very similar sources of
information. This result is supported by the fact that the
measurement error framework, which essentially describes the
forecasters as making numerous small mistakes while applying the same
procedure to the same data (recall Section \ref{ss:measurement}),
results in averaging-based aggregators.



Next, consider the second case, where the forecasters' information
sets do not overlap, i.e. $|B_{i} \cap B_{j}| = 0$ for all $i \neq j$. Such
an information structure is likely to arise if a team of forecasters
strategically decide to access and study disjoint sources of
information. The resulting information structure $\Sigma_{22}$ is
diagonal, and hence coherent if and only if $\sum_{i=1}^N \delta_i
\leq 1$. In this case, the additive nature of the Gaussian process gives $\delta' = \sum_{i=1}^N \delta_i$ and $X_{B'} = \sum_{i=1}^N X_{B_i}$. Consequently, the revealed aggregator coincides with the oracular aggregator. That is,
%Given that under non-overlapping information 
%$\delta' = \sum_{j=1}^N \delta_j$ and $X_{B'} = \sum_{j=1}^N X_{B_j}$, 
 \begin{align*}
p' &= p'' =  \Phi\left( \frac{\sum_{i=1}^N X_{B_i}}
  {\sqrt{1- \sum_{i=1}^N \delta_i}} \right) 
\end{align*}
This aggregator can be described in two steps: The first step consists
of voting, or range voting to be more specific (see, e.g.,
\citealt{fishkin1997voice}), where the votes are weighted according to
the importance of the forecasters' private information. This is
performed by the summation in the numerator. If this sum falls below
$0.0$ (or above $0.0$), the consensus believes that the event will not
happen (or will happen). The second step is made by the
denominator that extremizes the consensus according to the total
amount of information in the group. For instance, if the forecasters
know all the information, i.e. $\sum_{i=1}^N \delta_i = 1$, their vote
deterministically indicates whether the event $A$ happens or not. This
technique clearly leads to very extreme aggregate forecasts. Therefore
more extreme techniques, such as voting, can be expected to work well
if the forecasters use very different information sets.

%Combining this with our earlier discussion leads to the following observation.
In summary, the analysis suggests a spectrum of aggregators indexed by
the information overlap.  The optimal aggregator undergoes a smooth
transformation from averaging to summing of the probit forecasts as
the information overlap decreases from full overlap towards the point
of no overlap.
%This is summarized in the following observation.
%%\begin{observation}
%The information structures index a spectrum of aggregators. 
That is,
\begin{center}
\begin{tabular}{ccc}
%\stackrel{\text{ \normalsize full information overlap}}{\stackrel{\text{\normalsize no extremization}}{averaging}} && \Leftrightarrow  && \stackrel{\text{ no information overlap}}{\text{full extremization}}
%high information overlap & \multirow{2}{*}{$\Leftrightarrow$} & low information overlap\\
high information overlap & & low information overlap\\
low extremization & {\Large $\Longleftrightarrow$} & high extremization \\
averaging  & & voting\\
\end{tabular}
\end{center}
%\end{observation}
This observation gives qualitative guidance in real-world settings
where the general level of overlap can be said to be high or low.  For
instance, predictions from a group of forecasters working together or in
close collaboration can be averaged while predictions from a diverse
group of forecasters working independently of each other should be
aggregated via more extreme techniques such as voting (see
\citealt{parunak2013characterizing} for a discussion on voting-like techniques). This spectrum is particularly clear in the next section
that visualizes the intermediate scenarios with partial information
sharing among the forecasters.

%Because $p_i$ and $X_{B_i}$ are related by the one-to-one 
%transformation~(\ref{Indiv}), we may also write this as
% \begin{align}
%p' &= \Phi \left \{ \frac{1}{\sqrt{1 - \sum_{i=1}^N \delta_i}}
%   \; \sum_{i=1}^N \sqrt{1 - \delta_i} \Phi^{-1} (p_i) \right \} \, .
%\end{align}
%In the appendix we prove the following result.  Note that this 
%result is not distributional, it holds samplewise when all the
%forecasts fall on the same side of $1/2$.
%
%\begin{proposition}
%\label{positiveThmVote}
%Under the non-overlapping information structure, the extremization
%parameter $\alpha$ is at least $1$ either if every $X_{B_i} \geq 0$ 
%or if every $X_{B_i} \leq 0$ (equivalently, if every $p_i \geq 1/2$ 
%or every $p_i \leq 1/2$).
%\end{proposition}


\subsection{Partially Overlapping Information}
\label{compound}
A key quantity in analyzing extremizing is the oracle's amount of information $\delta'$. This value, however, can be only
determined from $\Sigma_{22}$ when $N = 2$. In this case, $\delta' = \delta_1 + \delta_2 - \rho_{12}$. Fortunately, $\delta'$ can be bounded for general $N$ by referring to the historical Boole Problem (\citealt{boole1854investigation}): 

\begin{quote}
Consider $N$ events $A_i$ for $i = 1, \dots, N$. Denote the marginal probabilities  with $p_i := \P(A_i)$ and the joint probabilities with $p_{ij} := \P(A_i \cap A_j)$. What is the best estimation of $\P(A_1 \cup \dots \cup A_N)$ in terms of $p_i$'s and $p_{ij}$'s?
\end{quote}
As was mentioned earlier in Remark (\ref{item:pool}), representing the pool of information with the unit interval $S = [0,1]$ allows us to interpret $\delta_i$'s as marginal probabilities for some $N$ events and $\rho_{ij}$'s as their joint probabilities. Therefore any approximation to the Boole Problem can be used to bound $\delta'$.  Fortunately, many such bounds have been suggested in the past. For
instance, 
%according to \cite{deza1997geometry}  several authors, including \cite{chung1941probability, dawson1967inequality, galambos1977bonferroni}, 
\cite{galambos1977bonferroni}
discuss the lower bound
\begin{align*}
\delta'_{min} := \frac{2}{m+1} \sum_{i=1}^N \delta_i - \frac{2}{m(m+1)} \sum_{1 \leq i < j \leq N} \rho_{ij}   \leq \delta',
%\delta'_{min} := \frac{N \sum_{i=1}^N \delta_i - 2 \sum_{1 \leq i < j \leq N} \rho_{ij}}{\lfloor \frac{N+1}{2} \rfloor \lceil \frac{N+1}{2} \rceil }  \leq \delta' \leq \frac{N \sum_{i=1}^N \delta_i - 2 \sum_{1 \leq i < j \leq N} \rho_{ij}}{N} := \delta'_{max}
\end{align*}
where 
\begin{align*}
m  &= \left\lfloor 2   \left( \sum_{1 \leq i < j \leq N} \rho_{ij} \right) \bigg/ \left(  \sum_{i=1}^N \delta_i \right) \right\rfloor + 1
\end{align*}
According to \cite{dawson1967inequality}, who analyze the same result but without the extremal property, this lower bound can be attained and hence cannot be improved. See \cite{deza1997geometry} for an excellent overview of upper and other lower bounds. 

%In this section, however, only a lower bound is of interest because it facilitates the analysis of the modal amount of extremization under general $N$.

%where
%\begin{align*}
%\delta'_{min} &= \frac{N \sum_{i=1}^N \delta_i - 2 \sum_{1 \leq i \leq j \leq N} \rho_{ij}}{\lfloor \frac{N+1}{2} \rfloor \lceil \frac{N+1}{2} \rceil }\\
%\delta'_{max} &=  \leq \delta' \leq \frac{N \sum_{i=1}^N \delta_i - 2 \sum_{1 \leq i \leq j \leq N} \rho_{ij}}{N}
%\end{align*}
%Note that both of these bounds reduce to $\delta_1 + \delta_2 -
%\rho_{12}$ when $N=2$. In this section, the lower bound is of particular interest because
%it allows the modal amount of extremization to be bounded from below under general $N$. See \citealt{deza1997geometry} for the further discussion of these and other
%bounds.

This main goal of this subsection is to develop intuition by visualizing extremization as a function of information overlap. The first step is to reduce the number of parameters in $\Sigma_{22}$. One natural approach is to assume that the
information sets have the same size and that the amount of overlap
between any two sets is constant, i.e., $|B_{1}| = \dots =
|B_{N}|$ and $|B_{i} \cap B_{j}| = |B_{h} \cap B_{k}|$ for all $i \neq
j$ and $h \neq k$.
%Here, we compare $\probit$ to the %oracular aggregator; in Section~\ref{compound2} 
%we compare the revealed aggregator to the probit opinion pool.  
This would hold in expectation if, for example, the
forecasters sampled information sources from a common distribution.
%The resulting form of $\Sigma_{22}$ is called \textit{compound
%symmetric}. 
Incorporating these assumptions in the multivariate Gaussian distribution
(\ref{oracleN}) gives
% $\rho \in [\max \{(N-T)/(T(N-1)), 0\},1] = A_\rho$. 
\begin{align*}
\left(\begin{matrix} X_{S} \\ X_{B'}\\ X_{B_1}\\ \vdots \\ X_{B_N} \end{matrix}\right) &\sim \mathcal{N}\left( 
 \boldsymbol{0}, 
% \left(\begin{matrix} 
%\Sigma_{11}' & \Sigma_{12}'\\
%\Sigma_{21}' & \Sigma_{22}\\
% \end{matrix}\right) 
% =
 \left(\begin{array}{cc|cccc}
1 & \delta'& \delta & \delta & \dots & \delta  \\ 
\delta' & \delta' & \delta & \delta & \dots & \delta  \\ \hline
\delta & \delta &\delta & \lambda\delta & \dots & \lambda\delta   \\ 
\delta& \delta & \lambda\delta & \delta & \dots & \lambda\delta  \\ 
\vdots &\vdots & \vdots & \vdots & \ddots & \vdots  \\ 
\delta &\delta & \lambda\delta & \lambda\delta & \dots & \delta\\ 
 \end{array}\right)\right)
\end{align*}
where $\delta$ is the amount of information known by a forecaster and
$\lambda$ is the proportion of this information that is shared with another forecaster.  

To ensure that
$\Sigma_{22}$ is coherent, the overlap parameter $\lambda$ must be constrained. 
First, $\lambda \leq 1$ because under any combination of $\delta$ and $N$ all
forecasters may know the same information. Second, note that 
information overlap is unavoidable when $\delta > 1/N$ and that minimum
sharing occurs when all information is known either to everyone or
to a single forecaster.  In other words, if $\delta > 1/N$ and
$B_{i} \cap B_j = B$ with $|B| = \lambda \delta$ for all $i \neq j$,
the value of $\lambda$ is minimized when $\lambda\delta + N(\delta -
\delta\lambda) = 1$.  Therefore the lower bound for $\lambda$ is $\max
\left\{ \frac{N-\delta^{-1}}{N-1}, 0\right\}$, and $\Sigma_{22}$ is
coherent if and only if
\begin{align}
\delta \in [0,1] &&  \lambda &\in \left[  
   \max \left\{ \frac{N-\delta^{-1}}{N-1}, 0\right\}, 1 \right), 
   \label{rhoDomain}
\end{align}
where the open upper bound on $\lambda$ ensures a non-singular $\Sigma_{22}$.

Under these simplifications, the extremization ratio (\ref{alpha})
becomes
\begin{align*}
\alpha &= \frac{X_{B'}}{\frac{1}{N}\sum_{j=1}^N X_{B_j}} \sqrt{\frac{1-\delta}{1-\delta'}} \sim \text{Cauchy}(x_0, \gamma)
\end{align*}
 with
\begin{align*}
x_0 &= \frac{N}{1+(N-1)\lambda}  \sqrt{\frac{1-\delta}{1-\delta'}} &&& \text{ and } &&& \gamma &=  \sqrt{\frac{N(\delta' + \delta' \lambda (N-1) - \delta N)}{\delta (\lambda (N-1) + 1)^2}}\sqrt{\frac{1-\delta}{1-\delta'}}
\end{align*}
The location parameter $x_0$ can be lower bounded by replacing
$\delta'$ with $\delta'_{min}$ when $N > 2$. Making this substitution and recalling
Proposition \ref{positiveProbThm} gives us
\begin{align}
x_{min} := \max\left\{ \frac{N}{1+(N-1)\lambda}  \sqrt{\frac{1-\delta}{1-\delta'_{min}}}, 1 \right\} \leq x_0 \label{bound}
\end{align}

\begin{figure}[t]
\hspace{-1.2em}
    \centering
%    \begin{subfigure}[b]{0.3\textwidth}
%        \includegraphics[width=\textwidth, height = \textwidth]{lwdbound}
%\caption{$N = 2$}	
%\label{xOracle2}
%    \end{subfigure}%
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth, height = \textwidth]{ExtremeX0N2}
\caption{$N = 2$}	
\label{xOracle2}
    \end{subfigure}%
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth, height = \textwidth]{ExtremeX0N10}
\caption{$N = 10$}	
\label{xOracle10}
    \end{subfigure}%
    \caption{ The extremization ratio follows a 
Cauchy$(x_0, \gamma)$, where $x_0$ is a location parameter and $\gamma$ 
is a scale parameter.  This figure plots $\log(x_0)$ for $N = 2$ and $\log(x_{min})$ for $N = 10$ forecasters.}
        \label{LevelplotsOracle}
\end{figure}

Figure \ref{LevelplotsOracle} shows $\log(x_0)$ and $\log(x_{min})$ for $N = 2$ and $N = 10$, respectively, under all plausible combinations of $\delta$ and $\lambda$. High values have been censored to keep the scale manageable.  Note that the results presented in Figure \ref{xOracle2} are completely general, apart from the assumption $\delta_1 = \delta_2$. Relaxing this assumption does not change the qualitative nature of the results. Overall, heavier extremizing is required when $\delta$ and $N$
are high but $\lambda$ is low. Given that $\delta'$ increases in
$\delta$ and $N$ but decreases in $\lambda$, the amount of extremizing
can be expected to increase in the total amount of information among
the forecasters $\delta'$.  This, however, does not provide a full
explanation of extremizing.  Information diversity is also an
important yet separate determinant.  To see this,
consider Figure \ref{xOracle2} and observe that fixing $\delta'$ to
some constant defines a curve $\lambda = 2 - \delta'/\delta$. For
instance, letting $\delta' = 1$ gives the boundary curve on the right
side of the plot.  This curve then shifts inwards and rotates slightly
counterclockwise as $\delta'$ decreases.  At the top end of each such curve
all forecasters know and share the total information, i.e. $\delta =
\delta'$ and $\lambda = 1.0$.  At the bottom end, on the other hand,
the forecasters partition the total information, i.e. $\delta =
\delta'/2$, and share nothing, i.e. $\lambda = 0.0$.  Given that
moving down along these curves increases both information diversity and
$x_0$, we conclude that information diversity is an important
determinant of extremizing together with the group's total amount of
information. This observation can guide practitioners towards proper extremization
because many aspects of a prediction setting can be
related to these two factors. For instance, extremization can be
expected to increase in factors such as number of forecasters,
subject-matter expertise, and human diversity, but to decrease in
collaboration, sharing of resources, and problem difficulty.

 
 
%
%\subsection{Partially Overlapping Information}
%\label{compound}
%The total amount of information among the forecasters $\delta'$ is
%uniquely determined by $\Sigma_{22}$ only when the group involves two
%forecasters. More specifically, $\delta' = \delta_1 + \delta_2 -
%\rho_{12}$ when $N=2$. By further assuming that $\delta = \delta_1 =
%\delta_2$, both $x_0$ and $\gamma$ depend only on two parameters and
%hence can be analyzed graphically. The results remain qualitatively
%similar even without assuming $\delta_1 = \delta_2$. This assumption,
%however, leads to fewer plots with essentially no loss of
%information. Let $\rho_{12} = \lambda\delta$, where the value of
%$\lambda$ is the proportion of the information known to one forecaster
%that is also known to the other forecaster. Under these assumptions
%the information structure $\Sigma_{22}$ is coherent if and only if
%\begin{align*}
%\delta \in [0,1] &&  \lambda &\in \left[  
%   \max \left\{ 2-\delta^{-1}, 0\right\}, 1 \right] 
%\end{align*}
%%  When $N=2$, the information structure is coherent if and only if 
%%\begin{align}
%% \delta_1, \delta_2 \in [0,1] && \rho_{12} \in \left[  \max \left\{\delta_1+\delta_2 - 1,  0\right\}, \min \left\{\delta_1, \delta_2 \right\} \right] \nonumber
%%\end{align}
%%The analysis is almost fully general because compound symmetry under $N = 2$ forecasters is equivalent to only assuming $\delta_1 = \delta_2$. 
%
%
%\begin{figure}[t]
%\hspace{-1.2em}
%    \centering
%    \begin{subfigure}[b]{0.33\textwidth}
%        \includegraphics[width=1.07\textwidth, height = \textwidth]{ExtremeX0}
%\caption{$\log(x_0)$}	
%\label{xOracle}
%    \end{subfigure}%
%\hspace{0.6em}
%    \begin{subfigure}[b]{0.33\textwidth}
%        \includegraphics[width= 0.95\textwidth, height = \textwidth]{ExtremeGamma}
%\caption{$\gamma$}
%\label{gammaOracle}
%        \end{subfigure}
%\hspace{-1.3em}
%    \begin{subfigure}[b]{0.33\textwidth}
%        \includegraphics[width=1.07\textwidth, height = \textwidth]{Probs}
%\caption{$\P(\alpha > 1 | \Sigma_{22}, \delta')$}
%\label{probOracle}
%        \end{subfigure}
%
%    \caption{ The amount of extremizing follows a 
%Cauchy$(x_0, \gamma)$, where $x_0$ is a location parameter and $\gamma$ 
%is a scale parameter.  This figure considers only $N = 2$ forecasters.}
%        \label{LevelplotsOracle}
%\end{figure}
% 
%Figure \ref{LevelplotsOracle} shows $\log(x_0)$, $\gamma$, and
%$\P(\alpha > 1 | \Sigma_{22}, \delta')$ under all plausible
%combinations of $\delta$ and $\lambda$. High values have been censored
%to keep the scale manageable. Notice that extremizing is required more
%often when $\delta$ is high and $\lambda$ is low.  Given that
%$\delta'$ increases in $\delta$ but decreases in $\lambda$, the amount
%of extremizing can be expected to increase in the total amount of
%information among the forecasters $\delta'$.  This, however, does not
%provide a full explanation of extremizing.  Information diversity is
%also an important yet separate determinant of extremizing.  To see
%this, observe that fixing $\delta'$ to some constant defines a curve
%$\lambda = 2 - \delta'/\delta$ on the plots in
%Figure~\ref{LevelplotsOracle}.  For instance, letting $\delta' = 1$
%gives the boundary curve on the right side of each plot.  This curve
%then shifts inwards and rotates slightly counterclockwise as $\delta'$
%decreases.  At the top end of each curve all forecasters know and
%share the total information, i.e. $\delta = \delta'$ and $\lambda =
%1.0$.  At the bottom end, on the other hand, the forecasters partition
%the total information, i.e. $\delta = \delta'/2$, and share nothing,
%i.e. $\lambda = 0.0$.  Given that moving down along the curve
%increases both information diversity and the modal amount of
%extremizing, information diversity is an important determinant of
%extremizing together with the group's total amount of information.
%%This provides the basis for our second observation.
%%\begin{observation}
%Therefore extremization can be expected to increase in factors such as
%number of forecasters, subject-matter expertise, and human diversity,
%but to decrease in collaboration, sharing of resources, and problem
%difficulty.
%%\begin{center}
%%\begin{tabular}{l | l}
%%increase in & decrease in\\ \hline
%%Number of forecasters & Collaboration\\
%%Subject-Matter forecasterise & Sharing of Resources\\
%%Human Diversity & Problem Difficulty
%%\end{tabular}
%%\end{center}
%%\end{observation}
%
%%Therefore the amount of extremizing can be expected to increase 
%%in many factors such as human diversity, subject-matter forecasterise, 
%%and the extent to which forecasters can access information, but decrease 
%%through forecaster collaboration and sharing of resources. 
%



\section{Probability Aggregation}
\label{aggregation}

This section first derives the revealed aggregator $p''$ for the
general Gaussian model. By further assuming
exchangeability among the forecasts, the revealed aggregator can be
applied to any single pool of probabilities. After proving some
properties of this aggregator, it is
tested on real-world forecasts of one-time events. This a)
illustrates one approach to estimating the information structures in
practice; and b) provides some empirical evidence in favor of the
partial information model.
% We then apply 
%this to the symmetric case, for which the oracular aggregator 
%was discussed in Section~\ref{compound}.  The revealed aggregator, 
%unlike the oracular aggregator, can be applied in practice.  
%We prove an extremizing result for this aggregator.

\subsection{Revealed Aggregator for the Gaussian Model}

Referring back to the multivariate Gaussian distribution
(\ref{Nforecasters}), if $\boldsymbol{X} = (X_{B_1}, X_{B_2}, \dots,
X_{B_N})'$ is a column vector of length $N$ and $\Sigma_{22}$ is a
coherent overlap structure such that $\Sigma_{22}^{-1}$ exists, then
\begin{align*}
X_{S} | \boldsymbol{X} \sim \mathcal{N}\left(\bar{\mu}, \bar{\Sigma}\right), 
\end{align*}
where
\begin{align*}
\bar{\mu} &= \mu_1 + \Sigma_{12} \Sigma_{22}^{-1} 
  (\boldsymbol{X} - \boldsymbol{\mu}_2) 
  = \Sigma_{12} \Sigma_{22}^{-1} \boldsymbol{X} \\
 \bar{\Sigma}&= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} 
 = 1 - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}   \, .
\end{align*}
These expressions can be found directly from the formulas of
the conditional multivariate Gaussian distribution (see, 
e.g., \citealt{ravishanker2001first}). 
%\citealt[Result~5.2.10, page~156]{ravishanker2001first}). 
The revealed aggregator then becomes
\begin{eqnarray}
p'' & = & \P\left(A  | \F''\right)  \nonumber \\
& = & \P\left(X_{S} > 0 | \boldsymbol{X}\right) \nonumber \\
%&=& \Phi\left( \frac{\Sigma_{12} \Sigma_{22}^{-1} \boldsymbol{X} - \Phi^{-1}(1-\tilde{p})}
%   {\sqrt{1 - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}}}\right) 
&=& \Phi\left( \frac{\Sigma_{12} \Sigma_{22}^{-1} \boldsymbol{X}}
   {\sqrt{1 - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}}}\right) 
\label{GeneralAggregator} \, .
\end{eqnarray}
%Furthermore, if $\one_N$ is a column vector of ones and 
%$\boldsymbol{P} = (P_{B_1}, P_{B_2}, \dots, P_{B_N})'$, 
%then the extremization parameter for $p''$ with respect to
%$\probit$ is given by 
%\begin{align*}
%\alpha  &= \frac{N \Sigma_{12} \Sigma_{22}^{-1} 
%  \boldsymbol{X}}{\left(\boldsymbol{1}_N' \boldsymbol{P} \right) 
%  \sqrt{1 - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21}}}  \, .
%\end{align*}
% where $\tilde{p} = \P(A)$ is the prior probability discussed in Section \ref{prelim} and 
 where $\boldsymbol{X} =
\Phi^{-1}(\boldsymbol{p})\sqrt{1-\Sigma_{12}}$. A practical application
of this computation, however, requires values for the entries of $\Sigma_{22}$.
As  was discussed earlier in Remark (\ref{item:specific}), the information structure can be estimated in one of three ways: by
assumption, by estimation, or in a Bayesian manner. If one is to
assume a structure, the most natural and non-informative choice is the
symmetric one discussed in Section \ref{compound}. The next subsection analyzes the revealed aggregator (\ref{GeneralAggregator}) under this structure.

%\textcolor{red}{Discuss here Julia's first paper. Hypothesis: the aggregator does not drop out a person whose $B_1 \subset B_2$ unless $B_3$ is completely separate. In a sense, in such a case the first forecaster does not provide anything beyond the second forecaster. This is not surprising. }


%\section{Version 1}
\subsection{Aggregation under Symmetric Information}
\label{compound2}
%In this section we assume symmetry, meaning that ${\bf X}$ and
%$\Sigma$ are given by~\eqref{eq:symmetric}.  This assumption
%on the parameters of the model corresponds to
This section assumes a type of exchangeability among the forecasters.
While this is somewhat idealized, it is a reasonable choice in a
low-information environment where there is no historical or
self-report data to distinguish the forecasters.  The averaging
aggregators described in Section~\ref{sec:prior}, for instance, are
symmetric. Therefore, to the extent that they reflect an underlying
model, the model assumes exchangeability.

%Under the Gaussian partial information model, this implies that the
%forecasters' information sets have the same size and that the amount
%of overlap between any two information sets is constant, i.e.,
%$|B_{1}| = \dots = |B_{N}|$ and $|B_{i} \cap B_{j}| = |B_{h} \cap
%B_{k}|$ for all $i \neq j$ and $h \neq k$.  
%%Here, we compare $\probit$ to the %oracular aggregator; in Section~\ref{compound2} 
%%we compare the revealed aggregator to the probit opinion pool.  
%The information structure $\Sigma_{22}$ is called \textit{compound
%symmetric}.  This structure would hold, for example, if the
%forecasters sample information sources from a common distribution.
%Plugging this structure in the multivariate Gaussian distribution (\ref{Nforecasters}) gives
%\begin{align*}
%%\label{eq:symmetric}
%\left(\begin{matrix} X_{S} \\ X_{B_1}\\ \vdots \\ X_{B_N} 
%   \end{matrix}\right) 
%  &\sim \mathcal{N}\left( \boldsymbol{0}, \left(\begin{matrix} 
%\Sigma_{11} & \Sigma_{12}\\
%\Sigma_{21} & \Sigma_{22}\\
% \end{matrix}\right) 
% =
% \left(\begin{array}{c|cccc}
%1 & \delta & \delta & \dots & \delta  \\ \hline
%%\delta' & \delta' & \delta & \delta & \dots & \delta  \\ \hline
%\delta  &\delta & \lambda\delta & \dots & \lambda\delta   \\ 
%\delta & \lambda\delta & \delta & \dots & \lambda\delta  \\ 
%\vdots & \vdots & \vdots & \ddots & \vdots  \\ 
%\delta  & \lambda\delta & \lambda\delta & \dots & \delta\\ 
% \end{array}\right)\right),
%\end{align*}
%where $\delta$ is the amount of information known by a forecaster and
%$\lambda$ is the proportion of information
%known to one forecaster that is also known to another single
%forecaster.  %This minor change of parametrization %was made for the sake of simplifying some of the following expressions.  
%To ensure that
%$\Sigma_{22}$ is coherent, a restriction must be placed on $\lambda$.
%First, given that under any combination of $\delta$ and $N$ all
%forecasters may know the exact same information, the value of
%$\lambda$ is bounded from above by $1$. Second, observe that
%information overlap is unavoidable when $\delta > 1/N$.  The minimum
%sharing occurs when all information is either known by everyone or
%private to a single forecaster.  In other words, if $\delta > 1/N$ and
%$B_{i} \cap B_j = B$ with $|B| = \lambda \delta$ for all $i \neq j$,
%the value of $\lambda$ is minimized when $\lambda\delta + N(\delta -
%\delta\lambda) = 1$.  Therefore the lower bound for $\lambda$ is $\max
%\left\{ \frac{N-\delta^{-1}}{N-1}, 0\right\}$, and $\Sigma_{22}$ is
%coherent if and only if
%\begin{align}
%\delta \in [0,1] &&  \lambda &\in \left[  
%   \max \left\{ \frac{N-\delta^{-1}}{N-1}, 0\right\}, 1 \right), 
%   \label{rhoDomain}
%\end{align}
%where the strict upper bound on $\lambda$ ensures that
%$\Sigma_{22}$ is not singular.  
%In the following discussion, 
%however, this technical restriction is ignored and the case 
%$\lambda = 1$ is analyzed as a limiting case.
%Plugging these simplifications in (\ref{alpha}) gives 
%\begin{align*}
%\alpha &= \frac{X_{B'}}{\frac{1}{N}\sum_{j=1}^N X_{B_j}} 
%  \sqrt{\frac{1-\delta}{1-\delta'}} 
%\end{align*}
%This follows a $\text{Cauchy}(x_0, \gamma)$ distribution with
%\begin{align*}
%x_0 &= \frac{N}{1+(N-1)\lambda}  \sqrt{\frac{1-\delta}{1-\delta'}}\\[2ex]
%  \gamma &=  \sqrt{\frac{N(\delta' + \delta' \lambda (N-1) - \delta N)}
%  {\delta (\lambda (N-1) + 1)^2}}\sqrt{\frac{1-\delta}{1-\delta'}}
%\end{align*}


Under the Gaussian partial information model, this implies a compound
symmetric information structure (see Section \ref{compound}). The
general form of the revealed aggregator~(\ref{GeneralAggregator}) then
simplifies to
\begin{align}
p''
  &=\Phi\left(\frac{\frac{1}{(N-1)\lambda +1} 
  \sum_{i=1}^N X_{B_i} }{\sqrt{1- \frac{N\delta}{(N-1)\lambda +1} }}  
  \right), \label{CompoundAggre}
\end{align}
where  $X_{B_i} =
\Phi^{-1}(p_i)\sqrt{1-\delta}$. 
%Recall from section \ref{compound} that $\delta \in [0,1]$ can be 
%interpreted as the average amount of information known by an forecaster, 
%and $\lambda$ is the average proportion of the known information shared 
%between any two forecasters. 
The domain restriction (\ref{rhoDomain}) ensures that the term under
the square-root in (\ref{CompoundAggre}) is always non-negative. In
this case the revealed aggregator is not as good as the oracular
aggregator. In fact, the former is a conditional expectation of the latter.

Given these interpretations, it may at first seem surprising that the
values of $\delta$ and $\lambda$ can be estimated in practice.
Intuitively, the estimation relies on two key aspects of the model: a)
the further the forecast is from the non-informative prior (typically
at $0.5$), the more informed the forecaster is likely to be (see
Figure~\ref{marginals}); and b) the more similar any two forecasts
are, the more information overlap the corresponding forecasters are
likely to have. This provides enough leverage to estimate the
information structure via the maximum likelihood method.  Complete
technical details and instructions for this are provided in the
Appendix.  Besides exchangeability, the revealed aggregator
(\ref{CompoundAggre}) is based on very different modeling assumptions
than the averaging aggregators described in
Section~\ref{sec:prior}. The following proposition summarizes some of
the key properties of the revealed aggregator (\ref{CompoundAggre}).

\begin{proposition} \label{positiveThm}
Under symmetric information, 
\begin{enumerate}
\item[$(i)$] the probit extremization ratio between the revealed
aggregator $p''$ and the probit opinion pool $\probit$ is given by the
non-random quantity
\begin{align*}
\alpha(p'', \probit)
%&= \frac{\displaystyle{\frac{N\sqrt{1-\delta}}{(N-1)\lambda +1}}}
%  {\displaystyle{\sqrt{1- \frac{N\delta}{(N-1)\lambda +1} }}} 
 &=  \frac{\gamma \sqrt{1 - \delta}}{\sqrt{1-\delta\gamma}}
\end{align*}
where
$$\gamma = \frac{N}{(N-1)\lambda +1},$$
\item[$(ii)$] the revealed aggregator $p''$ extremizes the probit
opinion pool $\probit$ as long as $p_j \neq p_i$ for some $j \neq i$,
and
\item[$(iii)$] the revealed aggregator $p''$ can leave the convex hull
of the individual probability forecasts.
\end{enumerate}
\end{proposition}
This proposition suggest that the revealed aggregator
(\ref{CompoundAggre}) is appropriate for combining probability
forecasts. To test this claim, the next subsection applies it to a set
of real-world forecasts. The aggregation is done one problem at a time
without assuming any other information besides the probability
forecasts themselves. That is, the outcomes of the events are not
known. This way any performance improvements reflect better fit of the
underlying model.



\subsection{Real-World Forecasting Data}
\label{realData}
The Good Judgment Project (GJP) ( \citealt{mellers2014psychological,
ungar2012good}) is a research study that since 2011 has recruited
thousands of forecasters from professional societies, research
centers, and alumni associations.  These forecasters are given
questions about future international political events, estimate the
probability of each event, and update their predictions when they feel
the probabilities have changed.  The forecasters know that their
probability estimates are assessed for accuracy using Brier scores (a
squared error loss function).  In addition to receiving \$150 for
meeting minimum participation requirements that do not depend on
prediction accuracy, the forecasters receive status rewards for their
performance via leader-boards displaying Brier scores for the top 20
forecasters.  Every year the top 1\% percent of the forecasters are
selected to the elite group of ``super-forecasters''. The
super-forecasters work in groups to make highly accurate predictions
on the same events as the rest of the forecasters.

This section illustrates the revealed aggregator (\ref{CompoundAggre})
on forecasts made by the super-forecasters. These forecasters were
chosen to the group of super-forecasters during the first year of the
GJP. Our evaluation, however, only uses forecasts that were made
during the second year. Therefore, the individual forecasts are
likely, but not guaranteed, to be relatively good. This group
consisted of 44 super-forecasters making predictions about 123
geopolitical events with two possible outcomes. For instance, some of
the questions were
\begin{quote}\textit{
Will France withdraw at least 500 troops from Mali before 10 April 2013?
}\end{quote}
and
\begin{quote}\textit{
Will a banking union be approved in the EU council before 1 March 2013? 
}
\end{quote}
 The forecasters were allowed to update their forecasts as long as the
questions were active. Some questions were active longer than
others. In fact, the number of active days ranged from 3 to 284 days,
with a mean of 96 days. This paper, however, does not focus on dynamic
data. Therefore we only considered each forecaster's most
recent forecast in the first three days of a problem because this is
when most uncertainty is present.  
In the resulting
dataset, the number of forecasts per event ranged from 17 to 34
forecasts, with a mean of 24.2 forecasts. To avoid infinite log-odds
and probit scores, extreme forecasts $p_i = 0$ and $1$ have been
censored to $p_i = 0.001$ and $0.999$, respectively.
 
 
The results are presented in terms of the mean Brier scores (BS),
i.e. the quadratic loss. To make this more concrete, consider $K$
problems and let $p_k$ be the aggregate forecast for the event $Y_k$,
where $k = 1, \dots, K$. Then,
 \begin{align*}
BS &= \frac{1}{K} \sum_{k=1}^K (p_k - Y_k)^2
 \end{align*}
 This score can be decomposed into three parts: reliability (REL),
resolution (RES), and uncertainty (UNC). The decomposition assumes
that the aggregate forecast can only take discrete values $f_j \in
[0,1]$ with $j = 1, \dots, J$. Let $n_j$ be the number of times $f_j$
occurs, and denote the empirical frequency of the corresponding events
with $o_j$.  Let $\bar{o}$ be the overall empirical frequency of
occurrence, i.e. $\bar{o} = \frac{1}{K} \sum_{k=1}^K Y_k$. Then the
mean Brier score decomposes into
 \begin{align*}
BS &= REL - RES + UNC\\
&= \frac{1}{K} \sum_{j=1}^J n_j (f_j - o_j)^2 - \frac{1}{K} \sum_{j=1}^J n_j (o_j - \bar{o})^2 + \bar{o}(1-\bar{o})
 \end{align*}
 Small values of the reliability term suggest improved
calibration. The resolution term, on the other hand, describes how far
the aggregate forecasts are from the naive forecast given by the
average $\bar{o}$ (equal to $0.187$ in our case). The typical goal is
to seek for an aggregate forecast that maximizes resolution subject to
being calibrated (see, e.g., \citealt{gneiting2007probabilistic}). The
uncertainty term quantifies overall uncertainty among the events and
does not depend on the aggregate forecasts.
 
 % latex table generated in R 3.0.3 by xtable 1.7-3 package
% Tue Aug 26 11:03:33 2014
\begin{table}[t]
\centering
\begin{tabular}{rrrrr}
  \hline
Aggregator & BS & REL & RES & UNC \\ 
  \hline
$\bar{p}$ & 0.1306 & 0.0351 & 0.0565 & 0.1520 \\ 
 $\plog$ & 0.1246 & 0.0262 & 0.0536 & 0.1520 \\ 
 $\probit$ & 0.1241 & 0.0272 & 0.0551 & 0.1520 \\ 
 $p''$ & 0.1199 & 0.0198 & 0.0520 & 0.1520 \\ 
   \hline
\end{tabular}
\caption{The mean Brier scores (BS) with its three components, reliability (REL), resolution (RES), and uncertainty (UNC), for the aggregated super-forecasts.}
\label{BrierTable}
\end{table}

Table \ref{BrierTable} shows the Briers scores for the average
forecast $\bar{p}$, logarithmic opinion pool $\plog$, probit opinion
pool $\probit$, and the revealed aggregator under symmetric
information $p''$. No empirical approaches were considered for two
reasons: a) they do not reflect an actual model of forecasts, and b)
they require a training set with known outcomes and hence cannot be
applied to one-time events. As can be seen in Table \ref{BrierTable},
the average forecast $\bar{p}$ is clearly the worst performing aggregator. As
discussed earlier, the two opinion pools, $\probit$ and $\plog$, are
very similar. Consequently, they present almost identical scores. Even
though the revealed aggregator is the least resolved, it presents
greatly improved calibration and, therefore, achieves the lowest Brier
score among all the aggregators. This is certainly an encouraging
result. It is important to note, however, that the revealed aggregator
under the symmetric information is only the first attempt in partial
information aggregation. More elaborate information structures and
estimation procedures are very likely to lead to many further
improvements.

\section{Discussion}
\label{discussion}
This paper introduced a probability model for event forecasts made by
a group of forecasters.  The model allows for interpretation of some
of the existing work on forecast aggregation and also sheds light on
empirical approaches such as the {\em ad hoc} practice of
extremization.  The general model is more plausible on the micro-level
than any other model has been to date. Under this model, we provided
some general results. For instance, we showed that the
\textit{oracular aggregator} is more likely to give a forecast that is
more extreme than one of the common benchmark forecasts, namely the
probit opinion pool (Proposition~\ref{positiveProbThm}).  Even though
no real world aggregator has access to all the information of the
oracle, the result illuminates why extremization is almost certainly
called for.  We also performed more detailed analyses under various
specific model specifications such as zero and complete information
overlap (Section~\ref{disjoint}), and full symmetry of the information
structure (Section~\ref{compound}).  Even though the zero and complete
information overlap models are not realistic, except under a very
narrow set of circumstances, they form logical extremes that shed
light on what drives good aggregation. The symmetric model is somewhat
more realistic.  Based on its analysis, we constructed graphs that
illuminated the effect of model parameters on the optimal amount of
extremization (Figure~\ref{LevelplotsOracle}).  We also considered the
{\em revealed aggregator}, which is the best in-practice aggregation
under the partial information model.  We derived a general formula for
this aggregator (Equation~\ref{GeneralAggregator}), as well as its
specific formula under the assumption of symmetric information
(Equation~\ref{CompoundAggre}). This specific form was shown to 
outperform the common averaging aggregators on real-world forecasts 
of one-time events. 

It is interesting to relate our discussion to the many empirical
studies conducted by the Good Judgment Project (see Section
\ref{realData} for a brief introduction of the project).  Generally
extremizing has been found to improve the average aggregates
(\citealt{mellers2014psychological, satopaa, satopaa2014probability}).
The average forecast of a team of super-forecasters, however, often
requires very little or no extremizing.  This can explained by the
partial information model as follows.  The super-forecasters are
highly knowledgeable (i.e. they have a high $\delta$) and they also
work in groups (i.e. they have a high $\rho$ and $\lambda$).  In
Figure \ref{LevelplotsOracle} they are situated around the upper-right
corners where almost no extremizing is required.  In other words,
there is very little usable information not already used in each
forecast.  Their forecasts are highly convergent and are likely to be
already very near the oracular forecast.  The GJP forecast data also
includes self-assessments of expertise.  Not surprisingly, the greater
the self-assessed expertise, the less extremizing appears to have been
required.  This is consistent with our interpretation that high values
of $\delta$ and $\lambda$ lead to a lower extremization parameter.

The partial information framework offers many future research
directions.  One involves estimation of parameters.  It seems likely
that use of this model together with good procedures for estimating
the information overlap among two or more forecasters can lead to many
improvements in probability aggregations. Given a stream of forecasts
of reasonable length, it is possible to estimate $|B_i|$ and $|B_i
\cap B_j|$ with a maximum likelihood procedure akin to details
described in Appendix. In principle, the size of $B_i$ can be
estimated from the distribution of the forecasts $p_{ij}$ and the size
of $B_i \cap B_j$ from correlations between the $i$th and $j$th
forecast streams.  Estimation of higher order intersections seems more
dubious.  Computations with $N=3$ show that, at least in some cases,
higher order intersections are not relevant to the revealed
aggregator.  Theoretical results on the significance or insignificance
of higher order intersections in the information overlap structure
would be desirable.

Another promising avenue is the Bayesian approach.  Two forecasters
whose forecasts are very similar or perhaps identical are more likely
to have very similar information sets.  In a Bayesian model, one would
expect more extremization from the probit opinion pool when the set of
forecasts is varied than when it is relatively homogeneous.  We have
work in progress analyzing a Bayesian model, but there are many, many
reasonable priors on information structures.  This avenue should
certainly be pursued and the results tested against other high
performing aggregators.


\section*{Acknowledgments} 
This research was supported in part by NSF grant \# DMS-1209117 and in part by a research contract to the University
of Pennsylvania and the University of California from the Intelligence
Advanced Research Projects Activity (IARPA) via the Department of
Interior National Business Center contract number D11PC20061. The
U.S. Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions expressed herein are
those of the authors and should not be interpreted as necessarily
representing the official policies or endorsements, either expressed
or implied, of IARPA, DoI/NBC, or the U.S. Government.

%We deeply appreciate the project management skills and work of Terry
%Murray and David Wayrynen, which went far beyond the call-of-duty on
%this project.

\appendix 
\section*{Appendix: Technical Details}
\label{appendix}

\renewcommand{\thesubsection}{\Alph{subsection}}

\subsection{Proofs}
%\textit{Proof of Theorem \ref{positiveThmVote}.} Let $\boldsymbol{d} = \frac{1}{N}\left((1-\delta_1)^{-1/2}, (1-\delta_2)^{-1/2}, \dots, (1-\delta_N)^{-1/2}\right)'$. Assume without loss of generality that $\bar{P} > 0$. Then the average probit forecast is extremized if
%\begin{align}
% \bar{P}&\leq  \frac{\sum_{j=1}^N X_{B_j}}{\sqrt{1 - \sum_{j=1}^N \delta_j}} &\Leftrightarrow&& 0 \leq  \left\{  \left(1 - \sum_{j=1}^N \delta_j \right)^{-1/2} \boldsymbol{1}_N - \boldsymbol{d}' \right\} \boldsymbol{X} \label{votingproof}
%\end{align}
% As $N (1-\delta_j)^{1/2} \geq \left(1 - \sum_{j=1}^N \delta_j \right)^{1/2}$ for all $j = 1, \dots, N$, all the elements of $$\left(1 - \sum_{j=1}^N \delta_j \right)^{-1/2} \boldsymbol{1}_N - \boldsymbol{d}' $$ are non-negative. Therefore the right hand side of (\ref{votingproof}) is always non-negative. \qed
% \
% \\
%\noindent
\subsubsection{Proof of Proposition \ref{CorrelationPolytope}.}
Denote the set of all coherent information structures $\Sigma_{22}$ with $\mathcal{Q}_N$. Consider $\mathcal{Q}_N \ni \Sigma_{22}$ that is defined by the Borel sets $\{ B_i : i = 1, \dots, N\}$.  Therefore its information can be represented in a diagram. Keeping the diagram representation in mind, the unit interval $S$ can be partitioned into $2^N$ disjoint parts
\begin{align*}
C_\v := \cup_{i \in \v} B_i \setminus \cup_{i \notin \v} B_i,
\end{align*}
where $\v \subseteq \{1, \dots, N\}$ denotes a subset of forecasters and the union is over $i = 1, \dots, N$. Therefore each $C_\v$ represents information known only to the forecasters indexed by $\v$. Given that $\sum_{\v} |C_\v|  = 1$, it is possible to establish a linear function $L$ from the simplex
\begin{align*}
\Delta_N &:= \text{conv}\{  \boldsymbol{e}_\v : \v \subseteq \{1, \dots, N\} \}\\
&= \left\{ \boldsymbol{z} \in \mathbb{R}^{2^N} : \boldsymbol{z} \geq \boldsymbol{0}, \boldsymbol{1}'\boldsymbol{z} = 1 \right\}
\end{align*}
to the space of coherent information structures $\mathcal{Q}_N$. In particular, the linear function $L: \boldsymbol{z} \in \Delta_N \to \Sigma_{22} \in \mathcal{Q}_N$ is defined such that $\rho_{ij} = \sum_{\{i,j\} \subseteq \v} z_\v$ and $\delta_i =  \sum_{i \in \v} z_\v$. Therefore $L(\Delta_N) = \mathcal{Q}_N$. Furthermore, given that $\Delta_N$ is a convex polytope, 
\begin{align}
L(\Delta_N) &= \text{conv}\{  L(\boldsymbol{e}_\v) : \v \subseteq \{1, \dots, N\} \} \label{first_step}\\
&=  \text{conv} \left\{\boldsymbol{x}\boldsymbol{x}' : \boldsymbol{x}  \in \{0,1\}^N \right\} \nonumber\\
&= \text{COR}(N), \nonumber
\end{align}
which gives $\text{COR}(N) = \mathcal{Q}_N$. Equality (\ref{first_step}) follows from the basic properties of convex polytopes (see, e.g., \citealt{mcmullen1971convex}).  Each $\Sigma_{22} \in \text{COR}(N)$ has $\frac{N(N+1)}{2} = \binom{n+1}{2}$ parameters and therefore exists in $\binom{n+1}{2}$ dimensions. \qed
%To show that none of these vertices are interior, assume that some point $X_v = \boldsymbol{x}_v\boldsymbol{x}_v'$, where $x_i = \one_{i \in v}$, is interior. Then $X_v$ has to be a convex combination of $\{X_w : w \neq v\}$. That is, for some constants $\phi_w$ such that $\phi_w \geq 0$ and $\sum_w \phi_w = 1$,  
%\begin{align*}
%X_v &= \sum_w \phi_w X_w
%\end{align*}


\subsubsection{Proof of Proposition \ref{positiveProbThm}.}
\begin{align*}
x_0 &= \kappa \frac{\sigma_1}{\sigma_2} \\
 &= \frac{ \sum_{i=1}^N \frac{\delta_i}{\sqrt{1-\delta_i}}}{\sqrt{\delta'  \left\{ \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}\right\}}} \sqrt{\frac{\frac{\delta'}{1-\delta'}}{\frac{1}{N^2} \left\{ \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}\right\} }}\\
%&=  \frac{N \sum_{j=1}^N \frac{\delta_j}{\sqrt{1-\delta_j}}}{\sqrt{ \left( \sum_{j=1}^N \frac{\delta_j}{1-\delta_j} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}\right)}} \sqrt{\frac{\frac{1}{1-\delta'}}{ \left( \sum_{j=1}^N \frac{\delta_j}{1-\delta_j} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}\right) }}\\
%&=  \frac{N \sum_{j=1}^N \frac{\delta_j}{\sqrt{1-\delta_j}}}{ \sum_{j=1}^N \frac{\delta_j}{1-\delta_j} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}} \sqrt{\frac{1}{1-\delta'}}\\
&=  \frac{N \sum_{i=1}^N \frac{\delta_i}{\sqrt{(1-\delta_i)(1-\delta')}}}{ \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}}}
%&\geq  1
%&=  \frac{\frac{1}{N} \sum_{j=1}^N \frac{\delta_j}{\sqrt{(1-\delta_j)(1-\delta')}}}{\frac{1}{N^2} \left( \sum_{j=1}^N \frac{\delta_j}{1-\delta_j} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}} \right)}\\
\end{align*}
Given that all the remaining terms are positive, the location parameter $x_0$ is also positive. Next compare the $N$ terms with a given subindex $i$ in the numerator with the corresponding terms in the denominator. From $\delta' \geq \delta_i \geq \rho_{ij}$ it follows that 
\begin{align}
\frac{\delta_i}{1-\delta_i} = \frac{\delta_i}{\sqrt{(1-\delta_i)(1-\delta_i)}} &\leq \frac{\delta_i}{\sqrt{(1-\delta_i)(1-\delta')}} \label{ProofIneq1}\\
 \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}} &\leq \frac{\delta_i}{\sqrt{(1-\delta_i)(1-\delta')}} \label{ProofIneq2}
\end{align}
Therefore 
\begin{align*}
N \sum_{i=1}^N \frac{\delta_i}{\sqrt{(1-\delta_i)(1-\delta')}} \geq \sum_{i=1}^N \frac{\delta_i}{1-\delta_i} + 2 \sum_{i,j: i<j} \frac{\rho_{ij}}{\sqrt{(1-\delta_j)(1-\delta_i)}},
\end{align*}
which gives that $x_0 \geq 1$. Given that the Cauchy distribution is symmetric around $x_0$, it must be the case that $\P(\alpha \geq 1 | \Sigma_{22}, \delta') \geq 0.5$. Based on (\ref{ProofIneq1}) and (\ref{ProofIneq2}), the location $x_0 = 1$ only when all the forecasters know the same information, i.e. when $\delta_i = \delta'$ for all $i = 1, \dots, N$. Under this particular setting, the amount of extremizing $\alpha$ is non-random and always equal to $1.0$. Therefore $\P(\alpha \geq 1 | \Sigma_{22}, \delta') = 1.0$.  Any deviation from this particular information structure makes $\alpha$ stochastic, $x_0 > 1$, and hence $\P(\alpha \geq 1 | \Sigma_{22}, \delta') > 0.5$. If the forecasters' information sets partition the full information, the sum of their probits is always on the correct side of $0.0$. At the same time the oracle deterministically outputs the correct outcome. Consequently, $\alpha = +\infty$ and $\P(\alpha \geq 1 | \Sigma_{22}, \delta') = 1$. Thus $\P(\alpha > 1 | \Sigma_{22}, \delta') \in (0.5, 1.0]$ when $\delta_i \neq \delta'$ for some $i = 1, \dots, N$. \qed
%\noindent
%\textit{Proof of Proposition \ref{positiveThmVote}.} Let $\boldsymbol{d} = \frac{1}{N}\left((1-\delta_1)^{-1/2}, (1-\delta_2)^{-1/2}, \dots, (1-\delta_N)^{-1/2}\right)'$. Assume without loss of generality that $\bar{P} > 0$. Then the average probit forecast is extremized if
%\begin{align}
% \bar{P}&\leq  \frac{\sum_{j=1}^N X_{B_j}}{\sqrt{1 - \sum_{j=1}^N \delta_j}} &\Leftrightarrow&& 0 \leq  \left\{  \left(1 - \sum_{j=1}^N \delta_j \right)^{-1/2} \boldsymbol{1}_N - \boldsymbol{d}' \right\} \boldsymbol{X} \label{votingproof}
%\end{align}
% As $N (1-\delta_j)^{1/2} \geq \left(1 - \sum_{j=1}^N \delta_j \right)^{1/2}$ for all $j = 1, \dots, N$, all the elements of $$\left(1 - \sum_{j=1}^N \delta_j \right)^{-1/2} \boldsymbol{1}_N - \boldsymbol{d}' $$ are non-negative. Therefore the right hand side of (\ref{votingproof}) is always non-negative. \qed
%\\
%\\
\subsubsection{Proof of Proposition \ref{positiveThm}.}

\begin{enumerate}
\item[(i)] This follows from direct computation,
\begin{align}
\alpha &= \left( \frac{\frac{1}{(N-1)\lambda +1} 
  \sum_{i=1}^N X_{B_i} }{\sqrt{1- \frac{N\delta}{(N-1)\lambda +1} }} \right) \Big/ \left( \frac{1}{N} \sum_{i=1}^N \frac{X_{B_i}}{\sqrt{1-\delta}} \right) \nonumber \\
&=    \frac{\frac{N\sqrt{1-\delta}}{(N-1)\lambda +1} 
   }{\sqrt{1- \frac{N\delta}{(N-1)\lambda +1} }}, \label{CompoundAlpha}
\end{align}
which simplifies to the given expression after substituting in $\gamma$. This quantity does not depend on any $X_{B_i}$ and is therefore non-random.

\item[(ii)] For a given $\delta$, the amount of extremizing $\alpha$ is minimized when $(N-1)\lambda +1$ is maximized. This happens as $\lambda \uparrow 1$. Plugging this into (\ref{CompoundAlpha}) gives
\begin{align*}
\alpha &= \frac{\frac{N\sqrt{1-\delta}}{(N-1)\lambda +1}}{\sqrt{1- \frac{N\delta}{(N-1)\lambda +1} }}  \downarrow \frac{\sqrt{1-\delta}}{\sqrt{1-\delta }} = 1
\end{align*}
\item[(iii)] Assume without loss of generality that $\bar{P} > 0$. If $\max\{p_1, p_2, \dots, p_N \} < 1$, then  setting $\delta = 1/N$ and $\lambda = 0$ gives an aggregate probability $p'' = 1$ that is outside the convex hull of the individual probabilities.
\qed
\end{enumerate}

\subsection{Parameter Estimation under Symmetric Information}
%The information among $N=2$ forecasters is completely characterized by $\delta_1$, $\delta_2$, and $\rho_{12}$. The values of these parameters can be estimated via the maximum likelihood method.  To make this more explicit, observe that the Jacobian for the map $\boldsymbol{P} \to \Phi\left(\boldsymbol{P}\right) = (\Phi(P_1), \Phi(P_2))'$ is
%\begin{eqnarray*}
%J(\boldsymbol{P}) &=& (2\pi)^{-1} \exp \left( - \frac{\boldsymbol{P}' \boldsymbol{P}}{2}   \right) 
%\end{eqnarray*}
%%
%%$\boldsymbol{P} \sim \mathcal{N}_N\left(\boldsymbol{0}, \Sigma_{22} (1-\delta)^{-1}\right)$ and that
%Let $t = \Phi^{-1}(1-\tilde{p})$. Then, if $h(\boldsymbol{P})$ denotes the multivariate Gaussian density of $\boldsymbol{P} \sim \mathcal{N}_2\left(-t/\sqrt{1-\boldsymbol{\delta}}, \Sigma_{P}\right)$,
%%by the Inverse Function Theorem 
%the density for  $\boldsymbol{p} = (p_1, p_2)'$ becomes
%\begin{eqnarray*}
%&& f\left(\boldsymbol{p} | \delta_1, \delta_2, \rho \right) \\
% &=& h(\boldsymbol{P}) J(\boldsymbol{P})^{-1} \\
%% \bigg|_{\boldsymbol{P} = \Phi^{-1}(\boldsymbol{p})}\\
%%&=& \frac{(1-\delta)^{N/2}}{\sqrt{ |\Sigma_{22}|}} \exp\left( -\frac{1}{2} \boldsymbol{P}' (1-\delta)\Sigma_{22}^{-1} \boldsymbol{P} + \frac{\boldsymbol{P}' \boldsymbol{P}}{2}   \right)\\
%&=& \frac{1}{\sqrt{ \left|\Sigma_{P}\right|}} \exp\left[ -\frac{1}{2} \left[ \left( \boldsymbol{P} + t/\sqrt{1-\boldsymbol{\delta}}\right)' \Sigma_{P}^{-1}  \left(\boldsymbol{P} + t/\sqrt{1-\boldsymbol{\delta}}\right) -  \boldsymbol{P}'\boldsymbol{P}\right] \right],
%\end{eqnarray*}
%where $\Phi^{-1}(\boldsymbol{p}) = \boldsymbol{P}$. Assume that the forecasters participated in $K$ problems. If the forecasts given for the $k$th problem are denoted with $\boldsymbol{p}_k$, then the maximum likelihood estimates of $\delta_1$, $\delta_2$, and $\rho$ are obtained from
%\begin{align*}
%\left(\hat{\delta}_1, \hat{\delta}_2, \hat{\rho}\right) =& \argmax_{\delta_1, \delta_2, \rho} \sum_{k=1}^K \log  f\left(\boldsymbol{p}_k| \delta_1, \delta_2, \rho \right),\\
%& \text{s.t. } \nonumber \delta_1, \delta_2 \in [0,1] \text{ and } \rho \in \left[  \max \left\{\delta_1+\delta_2 - 1,  0\right\}, \min \left\{\delta_1, \delta_2 \right\} \right]
%\end{align*}
%This works well as long as the forecasters participate in the same set of problems. The forecasters, however, were allowed to choose their problems and therefore generally do not participate in the same set of problems. Disregarding any problems with partial participation would lead to a loss of information. Therefore it is necessary to handle the missing values in some manner. The first step is to assume that the forecasts are missing at random. The likelihood for a unpaired forecast, say $p_j$, then is the marginal likelihood of $p_j$
%\begin{align*}
%%&& f\left(p_1 | \delta_1, \delta_2, \rho \right) \\
% m(p_j | \delta_j) &= \sqrt{\frac{1-\delta_j}{\delta_j}} \exp  \left\{ \frac{\delta_jP_j^2- (1-\delta_j)(P_j + t/\sqrt{1-\delta_j})^2 }{2\delta_j}  \right\},
%%&=& \frac{1}{\sqrt{ \left|\Sigma_{P}\right|}}  \int_0^1  \exp\left[ -\frac{1}{2} \Phi^{-1}(\boldsymbol{p})'\Lambda \Phi^{-1}(\boldsymbol{p})  \right] dp_2\\
%%%&=& \frac{1}{\sqrt{ \left|\Sigma_{P}\right|}}   \int_0^1  \exp\left[ -\frac{1}{2} \left( \Phi^{-1}(p_1)^2\Lambda_{11} + \Phi^{-1}(p_1)\Phi^{-1}(p_2)(\Lambda_{12}+\Lambda_{21}) +  \Phi^{-1}(p_2)^2\Lambda_{22} \right) \right] dp_2\\
%%%&=& \frac{1}{\sqrt{ \left|\Sigma_{P}\right|}}    \exp\left[ -\frac{1}{2}  P_1^2\Lambda_{11} \right]  \int_0^1  \exp\left[ -\frac{1}{2} \left( P_1\Phi^{-1}(p_2)(\Lambda_{12}+\Lambda_{21}) +  \Phi^{-1}(p_2)^2\Lambda_{22} \right) \right] dp_2\\
%%&=& \frac{1}{\sqrt{ \left|\Sigma_{P}\right|}}    \exp\left[ -\frac{1}{2}  P_1^2\Lambda_{11} \right]  \int_{-\infty}^\infty  \exp\left[ -\frac{1}{2} \left( P_1 P_2(\Lambda_{12}+\Lambda_{21}) +  P_2^2\Lambda_{22} \right) \right] \phi(P_2) dP_2\\
%%%&=& \frac{1}{\sqrt{ 2\pi \left|\Sigma_{P}\right|}}    \exp\left[ -\frac{1}{2}  P_1^2\Lambda_{11} \right]  \int  \exp\left[ -\frac{1}{2} \left( P_2 P_1 (\Lambda_{12}+\Lambda_{21}) +  P_2^2\Lambda_{22} \right) - \frac{1}{2} P_2^2\right]  dP_2\\
%%%&=& \frac{1}{\sqrt{ 2\pi \left|\Sigma_{P}\right|}}    \exp\left[ -\frac{1}{2}  P_1^2\Lambda_{11} \right]  \int  \exp\left[ -\frac{1}{2} \left( P_2 P_1 (\Lambda_{12}+\Lambda_{21}) +  P_2^2(\Lambda_{22}+1) \right) \right]  dP_2\\
%%%&=& \frac{1}{\sqrt{ 2\pi \left|\Sigma_{P}\right|}}    \exp\left[ -\frac{1}{2}  P_1^2\Lambda_{11} \right]  \frac{2.50663 \exp\left(\frac{0.5P_1^2 \Lambda_{12}^2}{\Lambda_{22}+1}\right)}{\sqrt{\Lambda_{22}+1}}\\
%%&=& \frac{2.50663}{\sqrt{ 2\pi \left|\Sigma_{P}\right|(\Lambda_{22}+1)}}    \exp\left[ -\frac{1}{2}  P_1^2\left( \Lambda_{11} - \frac{\Lambda_{12}^2}{\Lambda_{22}+1}\right) \right] \\
%%\end{eqnarray*}
%%Assume that $ \Sigma_{P}^{-1} - B_2$ is invertible and let $\Lambda = ( \Sigma_{P}^{-1} - B_2)^{-1}$. Then,
%%\begin{eqnarray*}
%% f\left(p_1 | \delta_1, \delta_2, \rho \right) &=&  2\pi \sqrt{ \frac{|\Lambda|}{|\Sigma_{P}|}} \int_{\mathbb{R}} \frac{1}{2\pi \sqrt{|\Lambda|}} \exp\left( -\frac{1}{2} \boldsymbol{P}' \Lambda^{-1} \boldsymbol{P}  \right) dP_2\\
%% &=&  2\pi  \sqrt{ \frac{|\Lambda|}{|\Sigma_{P}|}} \phi(p_1 | 0, \Lambda_{11}),
%\end{align*}
%where $P_j = \Phi^{-1}(p_j)$. 
%%where $\Lambda_{11}$ is the $(1,1)$th element of $\Lambda$ and $\phi(\cdot | \mu, \sigma^2)$ represents the probability density function of a normal distribution with mean $\mu$ and variance $\sigma^2$.
%%By symmetry, the likelihood $f\left(p_2 | \delta_1, \delta_2, \rho \right)$ follows similarly.  
%Let $\mathcal{K}_{12}$ denote the set of problems in which both forecasters participated. Similarly, let $\mathcal{K}_1$ and $\mathcal{K}_2$ denote the sets of problems in which only Forecaster $1$ and $2$ participated, respectively. The final optimization task then becomes
%\begin{align*}
%\left(\hat{\delta}_1, \hat{\delta}_2, \hat{\rho}\right) =& \argmax_{\delta_1, \delta_2, \rho} \left\{ \sum_{k \in \mathcal{K}_{12}} \log  f\left(\boldsymbol{p}_k| \delta_1, \delta_2, \rho \right) +  \sum_{j = 1}^2 \sum_{k \in \mathcal{K}_{j}} \log  m\left(p_j| \delta_j \right)  \right\},\\
%& \text{s.t. } \nonumber \delta_1, \delta_2 \in [0,1] \text{ and } \rho \in \left[  \max \left\{\delta_1+\delta_2 - 1,  0\right\}, \min \left\{\delta_1, \delta_2 \right\} \right]
%\end{align*}
%The feasible set of parameter values is described by the intersection of four half spaces. Therefore this optimization problem can be efficiently solved via generic techniques such as the barrier method (see, e.g., \cite{boyd2009convex} for more information). 
%%Given that this optimization problem cannot be solved analytically, numerical methods must be used. The feasible set of parameter values is a convex set defined by the intersection of four half-spaces. Therefore the constraints are equivalent to a set of linear inequalities
%%\begin{align*}
%%-\rho &\leq 0\\
%%\delta_1+\delta_2-\rho-1 &\leq 0\\
%%\rho - \delta_1 &\leq 0\\
%%\rho - \delta_2 &\leq 0
%%\end{align*}
%%and an adaptive barrier method can be used to solve the maximization problem. 
%%\subsection*{A.3 Estimation of $\delta$ and $\lambda$}


The values of $\delta$ and $\lambda$ can be estimated via the maximum likelihood method. To make this more explicit, observe that the Jacobian for the map $\boldsymbol{P} \to \Phi\left(\boldsymbol{P}\right) = (\Phi(P_1), \Phi(P_2), \dots, \Phi(P_N))'$ is
\begin{eqnarray*}
J(\boldsymbol{P}) &=& (2\pi)^{-N/2} \exp \left( - \frac{\boldsymbol{P}' \boldsymbol{P}}{2}   \right) 
\end{eqnarray*}
%
%$\boldsymbol{P} \sim \mathcal{N}_N\left(\boldsymbol{0}, \Sigma_{22} (1-\delta)^{-1}\right)$ and that
Let $J_{N \times N}$ represent a $N\times N$ matrix of ones. If $h(\boldsymbol{P})$ denotes the multivariate Gaussian density of $\boldsymbol{P} \sim \mathcal{N}_N\left(\boldsymbol{0}, \Sigma_{22} (1-\delta)^{-1}\right)$,
%by the Inverse Function Theorem 
the density for  $\boldsymbol{p} = (p_1, p_2, \dots, p_N)'$ becomes
\begin{eqnarray*}
 f\left(\boldsymbol{p} | \delta, \lambda \right) &=& h(\boldsymbol{P}) J(\boldsymbol{P})^{-1} \bigg|_{\boldsymbol{P} = \Phi^{-1}(\boldsymbol{p})}\\
%&=& \frac{(1-\delta)^{N/2}}{\sqrt{ |\Sigma_{22}|}} \exp\left( -\frac{1}{2} \boldsymbol{P}' (1-\delta)\Sigma_{22}^{-1} \boldsymbol{P} + \frac{\boldsymbol{P}' \boldsymbol{P}}{2}   \right)\\
&\propto&\frac{(1-\delta)^{N/2}}{\sqrt{ \left|\Sigma_{22}\right|}} \exp\left[ -\frac{1-\delta}{2} \Phi^{-1}(\boldsymbol{p})' \Sigma_{22}^{-1}  \Phi^{-1}(\boldsymbol{p})  \right],\\
%&=& \frac{(1-\delta)^{N/2}}{\sqrt{ \left|\Sigma_{22}\right|}} \exp\left[ -\frac{1}{2} \Phi^{-1}(\boldsymbol{p})' \left\{ (1-\delta) \Sigma_{22}^{-1} - I_N \right\} \Phi^{-1}(\boldsymbol{p})  \right],
\end{eqnarray*}
%where $\Phi^{-1}(\boldsymbol{p}) =  (\Phi^{-1}(p_1), \Phi^{-1}(p_2), \dots, \Phi^{-1}(p_N))$. 
%Given that $\Sigma_{22}$ can be written in the form  $\Sigma_{22} = B_N (\delta-\lambda\delta) + J_{N \times N} \lambda\delta$
where
%The determinant and inverse of $\Sigma_{22}$ are
\begin{align*}
\left| \Sigma_{22}\right| &= (\delta(1- \lambda))^N \left(1+\frac{N \lambda}{1 - \lambda} \right) \nonumber\\
\Sigma_{22}^{-1} &= I_N \left(\frac{1}{\delta-\lambda\delta} \right) - J_{N \times N} \frac{\lambda}{(1-\lambda)\delta\{1+(N-1) \lambda\}} 
\end{align*}
See \citet{rao2009linear} and the supplementary material of \citet{dobbin2005sample} for the derivations of the determinant and inverse of $\Sigma_{22}$, respectively. The maximum likelihood estimates of $\delta$ and $\lambda$ are then obtained from
\begin{align*}
\left(\hat{\delta}, \hat{\lambda}\right) =& \argmax_{\lambda, \delta} \log  f\left(\boldsymbol{p}| \delta, \lambda \right),\\
& \text{s.t. } \nonumber \delta \in [0,1] \text{ and } \lambda \in \left[  \max \left\{ \frac{N-\delta^{-1}}{N-1}, 0\right\}, 1 \right)
\end{align*}
%\textcolor{red}{check if this is a convex program}
Given that this cannot be solved analytically, numerical methods such as a simple grid-search or the barrier method must be used to find $\hat{\delta}$ and $\hat{\lambda}$. 

% 
%\subsection*{A.3 Rotating the Calibration Curve}
%The calibrated probabilities follow
%\begin{align*}
%p_{jk} |Y_k &\stackrel{iid}{\sim} \begin{cases}
%2p_{jk}& \text{ if } Y_k = 1\\
%2(1-p_{jk})& \text{ if } Y_k = 0,
%\end{cases}
%\end{align*}
%To introduce under- and overconfidence, this density is rotated counter- or clockwise, respectively, by some amount $\theta \in \left[-(\frac{\pi}{2} - \tan^{-1}(2), \tan^{-1}(2)\right]$. 
%The rotated density for $p_{jk} |Y_k, \theta$ is
%\begin{align}
%f(p_{jk} |Y_k, \theta) &=
%% 1+(-1)^{Y_k}R(\theta)(0.5-p_{jk})
%\begin{cases}
%c(\theta)\left[1+R(\theta)(p_{jk}-0.5) \right]& \text{ if } Y_k = 1\\
%c(\theta)\left[1+R(\theta)(0.5-p_{jk})\right]& \text{ if } Y_k = 0,
%\end{cases}
%\label{rotatedDens}
%\end{align}
%where 
%\begin{align*}
%R(\theta) &= \frac{2\cos(\theta)-\sin(\theta)}{\cos(\theta) + 2\sin(\theta)},
%\end{align*}
%and $c(\theta)$ is a normalizing constant that depends on $\theta$.  The domain of $p_{jk}$  also depends on $\theta$. In particular, if $\theta \geq 0$, then $c(\theta) = 1$ and $p_{ij} \in [0,1]$. If, on the other hand, $\theta < 0$, then $c(\theta) = R(\theta)/2$ and $p_{ij} \in [l,u]$, where $l = 0.5-R(\theta)^{-1}$ and $u = 0.5+R(\theta)^{-1}$. Given that $\P(Y_{jk} = 1) = 0.5$, marginalizing out $Y_{jk}$ gives
%\begin{align*}
%m(p_{jk} | \theta) &= 0.5f(p_{jk} |Y_k=0, \theta) + 0.5f(p_{jk} |Y_k=1, \theta)\\
%&= c(\theta)
%\end{align*}
%Therefore $p_{jk} | \theta$ is uniform over its domain. It is possible to sample from (\ref{rotatedDens}) using the inverse transform sampling method. More specifically, to make a single draw, begin by generating a uniform random variable $u \sim \mathcal{U}[0,1]$. This can be turned into a draw $p$ from (\ref{rotatedDens}) via the following solution to a second degree polynomial 
%\begin{align*}
%p &= \frac{-B+\sqrt{B^2-4AC}}{2A},
%\end{align*}
%where 
%\begin{align*}
%A &= \begin{cases}
%(-1)^{1-Y_{jk}} R(\theta)/2 & \text{ if } \theta \geq 0\\
%(-1)^{1-Y_{jk}} R(\theta)^2/4& \text{ if } \theta < 0
%\end{cases}\\
%B &= \begin{cases}
%1+(-1)^{Y_{jk}} R(\theta)/2 & \text{ if } \theta \geq 0\\
%R(\theta)/2\left(1- (-1)^{1-Y_{jk}} R(\theta)/2\right)& \text{ if } \theta < 0
%\end{cases}\\
%C &= \begin{cases}
%-u & \text{ if } \theta \geq 0\\
%-R(\theta)l +R(\theta)^2/4(-1)^{1-Y_{jk}}l(1-l) - u & \text{ if } \theta < 0
%\end{cases}
%\end{align*}


%\bibliographystyle{Chicago}
%\bibliographystyle{plainnat}
\bibliographystyle{chicago}
\bibliography{biblio}		% expects file ''myrefs.bib''



\end{document}
